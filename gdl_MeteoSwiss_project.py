# -*- coding: utf-8 -*-
"""GDL_New.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfJwMjkpjVoJzZIge6tIGOqIIVPin4Uh

## Imports
Make sure that you import the same stuff, especially optuna
"""

# prompt: mount drive, install dask, netcdf4 xarray and other stuff

# Mount drive
from google.colab import drive
drive.mount('/content/drive')

!pip install dask netCDF4 xarray zarr cftime bottleneck --quiet
!pip install omegaconf --quiet
!pip install torch-spatiotemporal --quiet
!pip install scoringrules --quiet
import torch
!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y --quiet

!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet
!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet
!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html --quiet
!pip install git+https://github.com/pyg-team/pytorch_geometric.git --quiet

! pip install optuna --quiet

"""## Their stuff

I did not change much here, except for when we get the get graph method, note that there is a function commented out. That was the one that we used before. The graph kwargs are passed like before, so no change in that.
"""

import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from tsl.ops.similarities import top_k
import xarray as xr
from typing import Union, Optional
from omegaconf import ListConfig
import numpy as np
import matplotlib.pyplot as plt
import os


class XarrayDataset(Dataset):
    def __init__(self, input_data, target_data):

        self.input_data, self.input_denormalizer = self.normalize(np.transpose(input_data.to_array().data, (1,2,3,0)))

        # NOTE No transformation is applied to targets. If normalized ~(0,1), they can be negatve
        # which is incompatible with CRPS_LogNormal.
        self.target_data, self.target_denormalizer = np.transpose(target_data.to_array().data, (1,2,3,0)), lambda x : x


        self.t, self.l, self.s, self.f = self.input_data.shape
        self.tg = self.target_data.shape[-1]

    def normalize(self, data):
        data_mean = np.nanmean(data, axis=(0, 1, 2), keepdims=True)
        data_std = np.nanstd(data, axis=(0, 1, 2), keepdims=True)
        standardized_data = (data - data_mean) / data_std

        def denormalizer(x): # closure (alternative: use a partial)
            if isinstance(x, torch.Tensor):
                return (x * torch.Tensor(data_std).to(x.device)) + torch.Tensor(data_mean).to(x.device)
            return (x * data_std) + data_mean

        return standardized_data, denormalizer

    def get_baseline_score(self, score_fn):
        pass

    @property
    def stations(self):
        return self.s

    @property
    def forecasting_times(self):
        return self.t

    @property
    def lead_times(self):
        return self.l

    @property
    def features(self):
        return self.f

    @property
    def targets(self):
        return self.tg

    def __len__(self):
        return self.input_data.shape[0]  # Number of forecast_reference_time

    def __getitem__(self, idx):
        sample_x = self.input_data[idx]
        sample_y = self.target_data[idx]
        return torch.tensor(sample_x, dtype=torch.float), torch.tensor(sample_y, dtype=torch.float)


    def __str__(self):
        return f"Dataset: [time={self.t}, lead_time={self.l}, stations={self.s}, features={self.f}] | target dim={self.tg}\n"



def get_graph(lat, lon, knn=10, threshold=None, theta=None):

    def haversine(lat1, lon1, lat2, lon2, radius=6371):
        import math
        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])

        # Differences in coordinates
        delta_lat = lat2 - lat1
        delta_lon = lon2 - lon1

        a = math.sin(delta_lat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(delta_lon / 2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        distance = radius * c

        return distance
    n = lat.shape[0]
    dist = np.zeros((n,n))
    for i in tqdm(range(n), desc="Outer Loop Progress"):
        for j in tqdm(range(i, n), desc=f"Inner Loop Progress (i={i})", leave=False):
            s1_lon = lon[i]
            s1_lat = lat[i]

            s2_lon = lon[j]
            s2_lat = lat[j]

            d = haversine(lat1=s1_lat, lon1=s1_lon, lat2=s2_lat, lon2=s2_lon)
            dist[i,j] = d
            dist[j,i] = d

    # def gaussian_kernel(x, theta=None):
    #     if theta is None:
    #         theta = x.std()
    #     weights = np.exp(-np.square(x / theta))
    #     return weights

    def gaussian_kernel(x, theta=None):
        if theta is None or theta == "std":
            theta = x.std()
        elif theta == "median":
            # extract strictly off-diagonal entries
            i, j = np.triu_indices(dist.shape[0], k=1)
            d_off = dist[i, j]
            theta = np.median(d_off)
        elif theta == "factormedian":
            # extract strictly off-diagonal entries
            i, j = np.triu_indices(dist.shape[0], k=1)
            d_off = dist[i, j]
            theta = np.median(d_off)*0.5
        weights = np.exp(-np.square(x / theta))
        return weights

    adj = gaussian_kernel(dist, theta)

    adj = top_k(adj,knn, include_self=True,keep_values=True)

    if threshold is not None:
            adj[adj < threshold] = 0

    return adj

class PostprocessDatamodule():
    def __init__(self, train_dataset: XarrayDataset,
                 val_dataset: XarrayDataset,
                 test_dataset: XarrayDataset,
                 adj_matrix: np.ndarray = None):
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.test_dataset = test_dataset
        self.adj_matrix = adj_matrix
        self.num_edges = (self.adj_matrix != 0).astype(np.int32).sum() if adj_matrix is not None else 0


    def __str__(self) -> str:
        dm_str = "Data Module: \n\n"
        dm_str += "Train:\n"
        dm_str += str(self.train_dataset)
        dm_str += "Val:\n"
        dm_str += str(self.val_dataset)
        dm_str += "Test:\n"
        dm_str += str(self.test_dataset)

        dm_str += f"Number of edges = {self.num_edges}"
        return dm_str

def get_datamodule(ds: xr.Dataset,
                   ds_targets: xr.Dataset,
                   predictors: Union[list, ListConfig],
                   lead_time_hours: int,
                   val_split: float,
                   target_var: str,
                   test_start_date: str,
                   train_val_end_date: Optional[str] = None,
                   return_graph=True,
                   graph_kwargs=None) -> PostprocessDatamodule:
    """_summary_

    Args:
        ds (xr.Dataset): The input dataset.
        ds_targets (xr.Dataset): The target dataset.
        predictors (Union[list, ListConfig]): The variable names to be used as predictors.
        lead_time_hours (int): The number of hours considered for the forecasted window.
        val_split (float): The percentage in [0,1) to be used as validation.
        target_var (str): The (single) target variable.
        test_start_date (str): The day where the test set will start.
        train_val_end_date (Optional[str], optional): The day when train and validation end. If not provided it will be set to test_start_date - 1.
        Pick a date that ensures no data leakage, eg. by using a large enough gap. Defaults to None.
        graph_kwargs (_type_, optional): Arguments to be passed to the graph-generating function. Defaults to None.

    Returns:
        PostprocessDatamodule: A datamodule with the train/val/test splits.
    """
    if isinstance(predictors, ListConfig):
        predictors = list(predictors)
    test_datetime = np.datetime64(test_start_date)
    if train_val_end_date is None:
        train_val_datetime = test_datetime - np.timedelta64(1,'D')
    else:
        train_val_datetime = np.datetime64(train_val_end_date)

    print(f'Train&Val sets end at {train_val_datetime}')
    print(f'Test set starts at {test_datetime}')

    # Get input data and split
    input_data = ds[predictors]
    input_data = input_data.sel(lead_time=slice(None, np.timedelta64(lead_time_hours, 'h')))

    input_data_train_val = input_data.sel(forecast_reference_time=slice(None, train_val_datetime))
    test_input_data = input_data.sel(forecast_reference_time=slice(test_datetime, None))

    train_val_rtimes = len(input_data_train_val['forecast_reference_time'])
    split_index = int(train_val_rtimes * (1.0 - val_split))

    train_input_data = input_data_train_val.isel(forecast_reference_time=slice(0, split_index))
    val_input_data = input_data_train_val.isel(forecast_reference_time=slice(split_index, None))

    # Get target data
    target_data = ds_targets[[target_var]]
    target_data = target_data.sel(lead_time=slice(None, np.timedelta64(lead_time_hours, 'h')))

    target_data_train_val = target_data.sel(forecast_reference_time=slice(None, train_val_datetime))
    test_target_data = target_data.sel(forecast_reference_time=slice(test_datetime, None))

    train_target_data = target_data_train_val.isel(forecast_reference_time=slice(0, split_index))
    val_target_data = target_data_train_val.isel(forecast_reference_time=slice(split_index, None))

    if return_graph:
        lat = ds.latitude.data
        lon = ds.longitude.data
        adj_matrix = get_graph(lat=lat, lon=lon, **graph_kwargs)
        return PostprocessDatamodule(train_dataset=XarrayDataset(input_data=train_input_data, target_data=train_target_data),
                                     val_dataset=XarrayDataset(input_data=val_input_data, target_data=val_target_data),
                                     test_dataset=XarrayDataset(input_data=test_input_data, target_data=test_target_data),
                                     adj_matrix=adj_matrix)
    return PostprocessDatamodule(train_dataset=XarrayDataset(input_data=train_input_data, target_data=train_target_data),
                                     val_dataset=XarrayDataset(input_data=val_input_data, target_data=val_target_data),
                                     test_dataset=XarrayDataset(input_data=test_input_data, target_data=test_target_data))



  ## LOSSES ###
import torch
import torch.nn as nn

class MaskedL1Loss(nn.Module):
    def __init__(self):
        super(MaskedL1Loss, self).__init__()

    def forward(self, predictions, targets):

        mask = ~torch.isnan(targets)

        predictions = predictions[mask]
        targets = targets[mask]

        # Compute the L1 loss (MAE) on the masked values
        loss = torch.abs(predictions - targets).mean()
        return loss

import torch
import torch.nn as nn
import scoringrules as sr
# import mlflow

class MaskedCRPSNormal(nn.Module):

    def __init__(self):
        super(MaskedCRPSNormal, self).__init__()

    def forward(self, pred, y):
        mask = ~torch.isnan(y)
        y = y[mask]
        mu = pred.loc[mask].flatten()
        sigma = pred.scale[mask].flatten()

        normal = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(sigma))

        scaled = (y - mu) / sigma

        Phi = normal.cdf(scaled)
        phi = torch.exp(normal.log_prob(scaled))

        crps = sigma * (scaled * (2 * Phi - 1) + 2 * phi - (1 / torch.sqrt(torch.tensor(torch.pi, device=sigma.device))))

        return crps.mean()

class MaskedCRPSLogNormal(nn.Module):

    def __init__(self):
        super(MaskedCRPSLogNormal, self).__init__()
        self.i = 0

    def forward(self, pred, y):
        mask = ~torch.isnan(y)

        y = y[mask]
        eps = 1e-5
        y += eps  # Avoid 0s (pdf(y=0) is undefined for  Y~LogNormal )

        mu = pred.loc[mask].flatten()
        sigma = pred.scale[mask].flatten()

        normal = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(sigma))

        # Source: Baran and Lerch (2015) Log‐normal distribution based Ensemble Model Output Statistics models for probabilistic wind‐speed forecasting
        omega = (torch.log(y)-mu)/sigma

        ex_input = mu + (sigma**2)/2

        # Clamp exponential for stability (e^15 = 3269017)
        # Note that the true mean of the Log-Normal is E[Y]=exp(mu+sigma^2/2), Y~LogN(mu,sigma)
        # This means that clamping this value still leaves room for a huge range of values
        # (Definitely enough for the wind speed :P)
        ex_input = torch.clamp(ex_input, max=15)
        # mlflow.log_metric('exp_input_debug', (ex_input).max(), step=self.i)
        self.i += 1

        ex = 2*torch.exp(ex_input)

        crps = y * (2*normal.cdf(omega)-1.0) - ex * (normal.cdf(omega-sigma)+normal.cdf(sigma/(2**0.5))-1.0)

        return crps.mean()


class MaskedCRPSEnsemble(nn.Module):

    def __init__(self):
        super(MaskedCRPSEnsemble, self).__init__()

    def forward(self, samples, y):
        # Pattern of y := [batch, time, station]
        # Patter of samples := [batch, time, station, sample]

        mask = ~torch.isnan(y)

        losses = sr.crps_ensemble(y.squeeze(-1), samples.squeeze(-1))

        return losses[mask.squeeze(1)].mean()


###

import torch.nn as nn
import torch
from abc import ABC, abstractmethod
from typing import Literal
import sys
from inspect import getmembers, isclass

class SoftplusWithEps(nn.Module):
    def __init__(self, eps=1e-5):
        super().__init__()
        self.softplus = nn.Softplus()
        self.eps = eps

    def forward(self, x):
        return self.softplus(x) + self.eps


class DistributionLayer(nn.Module, ABC):
    def __init__(self, input_size):
        super().__init__()

        self.distribution = getattr(torch.distributions, self.name)

        self.encoder = nn.Linear(input_size, self.num_parameters)

    @property
    @abstractmethod
    def num_parameters(self):
        pass

    @property
    @abstractmethod
    def name(self):
        pass

    @abstractmethod
    def process_params(self, x):
        pass


    def forward(self, x, return_type: Literal['samples', 'distribution']='distribution', reparametrized=True, num_samples=1):
        params = self.encoder(x)
        distribution = self.process_params(params)
        if return_type == 'distribution':
            return distribution
        return distribution.rsample((num_samples,)) if reparametrized else distribution.sample((num_samples,))


class LogNormalLayer(DistributionLayer):
    _name = 'LogNormal'
    def __init__(self, input_size):
        super(LogNormalLayer, self).__init__(input_size=input_size)
        self.get_positive_std = SoftplusWithEps()

    @property
    def name(self):
        return self._name

    @property
    def num_parameters(self):
        return 2

    def process_params(self, x):
        new_moments = x.clone()
        new_moments[...,1] = self.get_positive_std(x[...,1])

        log_normal_dist = self.distribution(new_moments[...,0:1], new_moments[...,1:2])
        return log_normal_dist


class NormalLayer(DistributionLayer):
    _name = 'Normal'
    def __init__(self, input_size):
        super(NormalLayer, self).__init__(input_size=input_size)
        self.get_positive_std = SoftplusWithEps()

    @property
    def name(self):
        return self._name

    @property
    def num_parameters(self):
        return 2

    def process_params(self, x):
        new_moments = x.clone()
        new_moments[...,1] = self.get_positive_std(x[...,1])

        normal_dist = self.distribution(new_moments[...,0:1], new_moments[...,1:2])
        return normal_dist


prob_layers = [obj[1] for obj in getmembers(sys.modules[__name__], isclass) if issubclass(obj[1], DistributionLayer) and obj[0] != 'DistributionLayer']
dist_to_layer = {
    l._name: l for l in prob_layers
}


# Models.py

from einops import rearrange
import torch.nn as nn
import torch
from typing import Literal
from tsl.nn.layers import GatedGraphNetwork, NodeEmbedding, BatchNorm
from tsl.nn.models import GraphWaveNetModel


class LayeredGraphRNN(nn.Module):
    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p = 0.1, mode: Literal['forwards', 'backwards'] = 'forwards', **kwargs) -> None:
        super().__init__(**kwargs)
        layers_ = []

        self.input_encoder = nn.Linear(input_size, hidden_size)

        for _ in range(n_layers):
            layers_.append(GatedGraphNetwork(input_size=hidden_size*2,
                                             output_size=hidden_size))


        self.mp_layers = torch.nn.ModuleList(layers_)
        self.state_size = hidden_size * n_layers
        self.n_layers = n_layers
        self.mode = mode
        self.dropout = nn.Dropout(p=dropout_p)

    def iterate_layers(self, state, x, edge_index):
        output = []
        state_ = rearrange(state, "b n ... (h l) -> l b n ... h", l=self.n_layers)
        for l, layer in enumerate(self.mp_layers):
            state_in_ = state_[l]

            input_ = torch.concatenate([state_in_, x], dim=-1) # recurrency
            input_ = self.dropout(input_)

            x = layer(input_, edge_index) # potential to do x += here
            if isinstance(x, tuple):
                x = x[0] # if cell is a GAT, it returns the alphas
            output.append(x)

        return torch.cat(output, dim=-1)


    def forward(self, x, edge_index):
        batch_size, win_size, num_nodes, num_feats = x.size()
        state = torch.zeros(batch_size, num_nodes, self.state_size, device=x.device)

        states = []
        # iterate forwards or backwards in time
        t0 = 0 if self.mode == 'forwards' else win_size - 1
        tn = win_size if self.mode == 'forwards' else -1
        step = 1 if self.mode == 'forwards' else -1

        for t in range(t0, tn, step):
            x_ = self.input_encoder(x[:,t])

            # iterate over the depth
            state_out = self.iterate_layers(state=state, x=x_, edge_index=edge_index)

            state = state_out + state # skip connection in time

            if self.mode == 'forwards':
                states.append(state)
            else:
                states.insert(0, state)


        return torch.stack(states, dim=1)


class BiDirectionalSTGNN(nn.Module):
    def __init__(self, input_size, hidden_size, n_stations, output_dist: str, n_layers=1, dropout_p = 0.1, **kwargs) -> None:
        super().__init__(**kwargs)



        self.encoder = nn.Linear(input_size, hidden_size)
        self.station_embeddings = NodeEmbedding(n_stations, hidden_size)
        self.forward_model = LayeredGraphRNN(input_size=hidden_size, hidden_size=hidden_size, n_layers=n_layers, mode='forwards', dropout_p=dropout_p)
        self.backward_model = LayeredGraphRNN(input_size=hidden_size, hidden_size=hidden_size, n_layers=n_layers, mode='backwards', dropout_p=dropout_p)

        self.output_distr = dist_to_layer[output_dist](input_size=hidden_size)

        self.skip_conn = nn.Linear(input_size, 2*hidden_size*n_layers)

        self.readout = nn.Sequential(
            nn.Linear(2*hidden_size*n_layers, hidden_size),
            BatchNorm(in_channels=hidden_size, track_running_stats=False),
            nn.SiLU(),
            nn.Dropout(p=dropout_p),
            nn.Linear(hidden_size, hidden_size)
        )


    def forward(self, x, edge_index):
        x0 = x
        x = self.encoder(x)
        x = x + self.station_embeddings()
        states_forwards = self.forward_model(x, edge_index)
        states_backwards = self.backward_model(x, edge_index)

        states = torch.concatenate([states_forwards, states_backwards], dim=-1)
        states = states + self.skip_conn(x0) # skip conn

        output = self.readout(states)

        return self.output_distr(output)



class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_dist: str, dropout_p, activation: str = "relu", **kwargs):
        super().__init__()

        activation_map = {
            "relu": nn.ReLU,
            "sigmoid": nn.Sigmoid,
            "tanh": nn.Tanh,
            "leaky_relu": nn.LeakyReLU
        }
        layers = []
        self.input_size = input_size
        for hs in hidden_sizes:
            layers.append(nn.Linear(input_size, hs))
            layers.append(activation_map[activation]())
            layers.append(nn.Dropout(p=dropout_p))

            input_size = hs
        self.layers = nn.Sequential(*layers)

        self.output_distr = dist_to_layer[output_dist](input_size=hidden_sizes[-1])

        self.skip_conn = nn.Linear(self.input_size, hidden_sizes[-1])

    def forward(self, x, **kwargs):
        # ignore edge index
        x_skip = self.skip_conn(x)

        x = self.layers(x)
        x = x + x_skip

        return self.output_distr(x)


class WaveNet(nn.Module):

    def __init__(self, input_size, time_steps, hidden_size, n_stations, output_dist, ff_size=256, n_layers=6, temporal_kernel_size=3, spatial_kernel_size=2, **kwargs):
        super().__init__()

        self.wavenet = GraphWaveNetModel(input_size=input_size,
                          output_size=hidden_size,
                          horizon=time_steps,
                          hidden_size=hidden_size,
                          ff_size=ff_size,
                          n_layers=n_layers,
                          temporal_kernel_size=temporal_kernel_size, spatial_kernel_size=spatial_kernel_size,
                          dilation=2, dilation_mod=3, n_nodes=n_stations)
        self.output_distr = dist_to_layer[output_dist](input_size=hidden_size)

    def forward(self, x, edge_index):
        output = self.wavenet(x, edge_index)

        return self.output_distr(output)


import torch
import torch.nn as nn
from einops.layers.torch import Rearrange
from tsl.nn.layers import GatedGraphNetwork, NodeEmbedding, BatchNorm

'''
These models are in the prototype phase and might not perform well yet.
'''

class CausalConv1d(nn.Conv1d):
    """Causal Convolution ensuring no information leakabe from future to past timesteps"""
    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):
        super().__init__(in_channels, out_channels, kernel_size, padding=0, dilation=dilation, **kwargs)
        self._padding = (kernel_size - 1) * dilation

    def forward(self, x):
        x = nn.functional.pad(x, (self._padding, 0))
        return super().forward(x)

class TCNLayer(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout_p=0.1, causal_conv=True):

        super().__init__()

        if causal_conv:
            self.cconv = CausalConv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, dilation=dilation)
        else:
            padding = int(dilation * (kernel_size -1) / 2) # Padding to keep the input shape the same as the output shape
            self.cconv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, dilation=dilation, padding=padding)
        self.bn = nn.BatchNorm1d(num_features=out_channels)
        self.act = nn.ReLU()
        self.dropout = nn.Dropout1d(p=dropout_p)
        self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        residual = x
        x = self.cconv(x)
        x = self.bn(x)
        x = self.act(x)
        x = self.dropout(x)
        residual = self.downsample(residual) # may be an identity

        return residual + x, x


class TCN_GNN(nn.Module):
    def __init__(self, num_layers, input_size, output_dist, hidden_channels, n_stations, kernel_size=3, dropout_p=0.2, causal_conv=True,**kwargs):
        super().__init__()

        tcn_layers = []
        gnn_layers = []
        norm_layers = []

        if isinstance(hidden_channels, int):
            hidden_channels = [hidden_channels]*num_layers
        assert len(hidden_channels) == num_layers

        self.rearrange_for_gnn = Rearrange('(b n) c t -> b t n c', n=n_stations)
        self.rearrange_for_tcn = Rearrange('b t n c -> (b n) c t')

        self.station_embeddings = NodeEmbedding(n_stations, hidden_channels[0])
        self.encoder = nn.Linear(input_size, hidden_channels[0])

        for l in range(num_layers):
            dilation = 2**l
            input_size = hidden_channels[0] if l == 0 else hidden_channels[l-1]
            tcn_layers.append(TCNLayer(in_channels=input_size,
                                       out_channels=hidden_channels[l],
                                       kernel_size=kernel_size,
                                       dilation=dilation, dropout_p=dropout_p, causal_conv=causal_conv))

            gnn_layers.append(GatedGraphNetwork(input_size=hidden_channels[l], output_size=hidden_channels[l]))
            norm_layers.append(BatchNorm(in_channels=hidden_channels[l]))

        self.tcn_layers = nn.ModuleList(tcn_layers)
        self.gnn_layers = nn.ModuleList(gnn_layers)
        self.norm_layers = nn.ModuleList(norm_layers)

        self.output_distr = dist_to_layer[output_dist](input_size=hidden_channels[-1])

    def forward(self, x, edge_index):

        x = self.encoder(x)
        x = x + self.station_embeddings()

        x = self.rearrange_for_tcn(x)

        skips = []
        for tcn_l, gnn_l, norm_l in zip(self.tcn_layers, self.gnn_layers, self.norm_layers):
            x, skip = tcn_l(x)          # Temporal convolution
            x = self.rearrange_for_gnn(x)
            x = gnn_l(x, edge_index)    # Spatial convolution \forall time steps t
            x = norm_l(x)
            x = self.rearrange_for_tcn(x)

            skips.append(skip)

        skips_stack = torch.stack(skips, dim=-2)
        result = skips_stack.sum(dim=-2) + x

        output = self.rearrange_for_gnn(result)

        return self.output_distr(output)

"""## My Model. Baseline

This is the baseline model I made, note that it uses the GRU architecture. It should be pretty plug and play.

# EnhancedGRUBaseline

EnhancedGRUBaseline takes as input a tensor
$$
  X \in \mathbb{R}^{B\times T\times N\times F}
$$
(batch size $B$, forecast length $T$, $N$ stations, $F$ features) and outputs a
LogNormal distribution over wind speeds at each station and lead time. Its key components:

We embed station identity via a learned matrix
$E\in\mathbb{R}^{N\times H}$, so that each station $s$ has embedding $e_s\in\mathbb{R}^H$.
Adding $e_s$ to the projected inputs breaks permutation invariance and lets the
network learn station‐specific biases (e.g.\ elevation effects).

Similarly we learn horizon embeddings
$$
  h_\ell\in\mathbb{R}^H,\quad \ell=0,\dots,T-1,
$$
which are added after the gated‐skip block to encode the systematic decay of
forecast skill with lead time.

A linear layer
$$
  W_{\mathrm{proj}}\!: \mathbb{R}^F\to\mathbb{R}^H
$$
projects raw predictors into a hidden space of dimension $H$.

We chose a bidirectional GRU because gated recurrent units efficiently capture
local temporal dynamics and handle vanishing gradients via update/reset gates.
Splitting the hidden size into $H/2$ forward and $H/2$ backward dimensions yields
a full state $h_t\in\mathbb{R}^H$ that integrates information from both past
and future within the window.

Immediately after the GRU we apply LayerNorm:
$$
  \mathrm{LN}(z_t)=\gamma\frac{z_t-\mu_z}{\sigma_z}+\beta,
$$
which stabilizes training by normalizing across the feature dimension, reducing
internal covariate shift and allowing larger learning rates.

To capture long‐range dependencies, we add a temporal multi‐head attention block:
$$
  \mathrm{Attn}(Q,K,V)
  = \mathrm{softmax}\!\bigl(\tfrac{QK^\top}{\sqrt{H/A}}\bigr)\,V,
$$
with $A$ heads operating over the time axis.  This augments the GRU’s local
memory with global context, letting the model re‐weigh past events adaptively.

A gated residual skip then fuses this attention output $a_t$ with the original
projected input $p_t$ via
$$
  g_t=\sigma(W_g a_t + b_g),\quad
  h_t = g_t\odot a_t + (1-g_t)\odot p_t,
$$
which both preserves raw ensemble statistics and improves gradient flow.

We reshape back to $(B,T,N,H)$ and perform station‐axis self‐attention:
$$
  \mathrm{Attn}\!\bigl(H_t\bigr),\quad H_t\in\mathbb{R}^{(B\,T)\times N\times H},
$$
so the model learns spatial correlations among stations in complex terrain.

Finally, a small linear “distribution head” outputs parameters
$(\mu,\sigma)$ for a LogNormal:
$$
  \mu=W_\mu^\top h + b_\mu,\quad
  \sigma=\mathrm{softplus}(W_\sigma^\top h + b_\sigma)+\varepsilon,
$$
ensuring $\sigma>0$.  Training minimizes the closed‐form CRPS for LogNormal
forecasts, which balances calibration and sharpness.
"""

##### MY MODEL BASELINE #####
import torch
import torch.nn as nn
from einops import rearrange, repeat
from einops.layers.torch import Rearrange

class EnhancedGRUBaseline(nn.Module):
    r"""
    # EnhancedGRUBaseline

    EnhancedGRUBaseline takes as input a tensor
    $$
      X \in \mathbb{R}^{B\times T\times N\times F}
    $$
    (batch size $B$, forecast length $T$, $N$ stations, $F$ features) and outputs a
    LogNormal distribution over wind speeds at each station and lead time. Its key components:

    We embed station identity via a learned matrix
    $E\in\mathbb{R}^{N\times H}$, so that each station $s$ has embedding $e_s\in\mathbb{R}^H$.
    Adding $e_s$ to the projected inputs breaks permutation invariance and lets the
    network learn station‐specific biases (e.g.\ elevation effects).

    Similarly we learn horizon embeddings
    $$
      h_\ell\in\mathbb{R}^H,\quad \ell=0,\dots,T-1,
    $$
    which are added after the gated‐skip block to encode the systematic decay of
    forecast skill with lead time.

    A linear layer
    $$
      W_{\mathrm{proj}}\!: \mathbb{R}^F\to\mathbb{R}^H
    $$
    projects raw predictors into a hidden space of dimension $H$.

    We chose a bidirectional GRU because gated recurrent units efficiently capture
    local temporal dynamics and handle vanishing gradients via update/reset gates.
    Splitting the hidden size into $H/2$ forward and $H/2$ backward dimensions yields
    a full state $h_t\in\mathbb{R}^H$ that integrates information from both past
    and future within the window.

    Immediately after the GRU we apply LayerNorm:
    $$
      \mathrm{LN}(z_t)=\gamma\frac{z_t-\mu_z}{\sigma_z}+\beta,
    $$
    which stabilizes training by normalizing across the feature dimension, reducing
    internal covariate shift and allowing larger learning rates.

    To capture long‐range dependencies, we add a temporal multi‐head attention block:
    $$
      \mathrm{Attn}(Q,K,V)
      = \mathrm{softmax}\!\bigl(\tfrac{QK^\top}{\sqrt{H/A}}\bigr)\,V,
    $$
    with $A$ heads operating over the time axis.  This augments the GRU’s local
    memory with global context, letting the model re‐weigh past events adaptively.

    A gated residual skip then fuses this attention output $a_t$ with the original
    projected input $p_t$ via
    $$
      g_t=\sigma(W_g a_t + b_g),\quad
      h_t = g_t\odot a_t + (1-g_t)\odot p_t,
    $$
    which both preserves raw ensemble statistics and improves gradient flow.

    We reshape back to $(B,T,N,H)$ and perform station‐axis self‐attention:
    $$
      \mathrm{Attn}\!\bigl(H_t\bigr),\quad H_t\in\mathbb{R}^{(B\,T)\times N\times H},
    $$
    so the model learns spatial correlations among stations in complex terrain.

    Finally, a small linear “distribution head” outputs parameters
    $(\mu,\sigma)$ for a LogNormal:
    $$
      \mu=W_\mu^\top h + b_\mu,\quad
      \sigma=\mathrm{softplus}(W_\sigma^\top h + b_\sigma)+\varepsilon,
    $$
    ensuring $\sigma>0$.  Training minimizes the closed‐form CRPS for LogNormal
    forecasts, which balances calibration and sharpness.
    """
    def __init__(
        self,
        input_size: int,
        hidden_channels: int,
        output_dist: str,
        n_stations: int,
        num_layers: int = 2,
        dropout_p: float = 0.1,
        attn_heads: int = 4,
        station_heads: int = 2,
        max_lead: int = 96,
    ):
        super().__init__()
        # station embeddings
        self.station_emb = nn.Embedding(n_stations, hidden_channels)

        # time embeddings
        self.horizon_emb = nn.Embedding(max_lead+1, hidden_channels)


        # project raw features to hidden dim
        self.input_proj = nn.Linear(input_size, hidden_channels)

        self.gru = nn.GRU(
            hidden_channels,
            hidden_channels // 2,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout_p if num_layers > 1 else 0.0,
            bidirectional=True,
        )

        # layer‐norm on hidden
        self.ln = nn.LayerNorm(hidden_channels)

        # temporal self‐attention
        self.attn = nn.MultiheadAttention(
            embed_dim=hidden_channels,
            num_heads=attn_heads,
            batch_first=True
        )

        # gated residual skip
        self.gate = nn.Linear(hidden_channels, hidden_channels)

        # skip from raw inputs
        self.skip_proj = nn.Linear(input_size, hidden_channels)

        # station‐axis self‐attention
        self.station_attn = nn.MultiheadAttention(
            embed_dim=hidden_channels,
            num_heads=station_heads,
            batch_first=True
        )

        self.output_distr = dist_to_layer[output_dist](input_size=hidden_channels)

        self.to_seq = Rearrange('b t n f -> (b n) t f')
        self.from_seq = Rearrange('(b n) t h -> b t n h', n=n_stations)

    def forward(self, x, edge_index=None):
        """
        x: [B, T, N, F]
        returns a torch.distributions.Distribution over [B, T, N]
        """
        B, T, N, F = x.shape

        h_in = self.to_seq(x)

        ids = repeat(torch.arange(N, device=x.device), 'n -> b n', b=B)
        ids = rearrange(ids, 'b n -> (b n)')

        emb = self.station_emb(ids)
        emb = emb.unsqueeze(1).expand(-1, T, -1)

        # project inputs + add station embedding
        h_proj = self.input_proj(h_in) + emb

        h_out, _ = self.gru(h_proj)
        h_out = self.ln(h_out)

        # temporal self-attention
        attn_out, _ = self.attn(h_out, h_out, h_out)
        h_temp = h_out + attn_out

        # gated residual skip
        gate = torch.sigmoid(self.gate(h_temp))
        skip = self.skip_proj(h_in)
        h_res = gate * h_temp + (1 - gate) * skip
        le = repeat(torch.arange(T, device=h_res.device), 't -> (b n) t', b=B, n=N)
        h_res = h_res + self.horizon_emb(le)


        h_final = self.from_seq(h_res)


        h2 = rearrange(h_final, 'b t n h -> (b t) n h')
        attn_stat, _ = self.station_attn(h2, h2, h2)
        attn_stat = rearrange(attn_stat, '(b t) n h -> b t n h', b=B, t=T)

        h_final = h_final + attn_stat

        h_final = self.from_seq(h_res)

        return self.output_distr(h_final)

"""## Magic

This is where the magic happens, I mask the anomalous targets here, and I also initialize the weights. Note init weight is a very important function.
"""

def mask_anomalous_targets(y, min_speed=0.1, max_speed=60.0):
    """
    y: tensor of shape [B, L, N, 1] (or [B, L, N]) containing observed wind speeds
    min_speed: speeds below this (e.g. frozen sensors) will be masked
    max_speed: speeds above this (physically implausible gusts) will be masked
    returns: a copy of y with out‐of‐range values replaced by NaN
    """
    squeezed = (y.squeeze(-1) if y.dim()==4 else y)
    bad = (squeezed < min_speed) | (squeezed > max_speed) | torch.isnan(squeezed)
    y_clean = squeezed.clone()
    y_clean[bad] = float('nan')
    return y_clean.unsqueeze(-1) if y.dim()==4 else y_clean



def init_weights(m):
    """
    Initialize module weights to promote stable and efficient training across diverse layer types.

    The Xavier (Glorot) uniform initialization for embeddings and fully connected layers preserves variance
    of activations and gradients when propagating through depth, preventing early saturation or vanishing/exploding
    gradients. Zeroing biases ensures no unwanted initial offsets. For convolutional layers, Kaiming (He) uniform
    initialization tailored to ReLU non-linearities maintains the forward signal’s scale and supports deeper
    temporal convolutional networks.

    Recurrent layers (GRUs) benefit from a hybrid scheme: input-to-hidden weights follow Xavier uniform to
    balance inputs, while hidden-to-hidden weights use orthogonal initialization. Orthogonal matrices preserve
    vector norms under repeated multiplication, which mitigates gradient decay or explosion over long sequences,
    improving temporal memory. All GRU biases start at zero to avoid biasing update gates at initialization.

    BatchNorm weights are set to one (identity scaling) and biases to zero so that it doesn't normalize it in the beginning,
    letting subsequent layers learn necessary adjustments. Multihead attention modules require careful splitting:
    the combined in_proj matrix is Xavier initialized to treat query, key, and value uniformly, and the output
    projection is also Xavier initialized, with all biases zeroed. A catch all clause ensures any custom layer
    with weight/bias attributes receives a sensible Xavier or zero initialization, avoiding uninitialized parameters.
    """
    if isinstance(m, nn.Embedding):
        nn.init.xavier_uniform_(m.weight)

    elif isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)

    elif isinstance(m, nn.GRU):
        for name, param in m.named_parameters():
            if "weight_ih" in name:
                nn.init.xavier_uniform_(param)
            elif "weight_hh" in name:
                nn.init.orthogonal_(param)
            elif "bias" in name:
                nn.init.zeros_(param)

    elif isinstance(m, nn.Conv1d):
        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)

    elif isinstance(m, nn.BatchNorm1d):
        nn.init.ones_(m.weight)
        nn.init.zeros_(m.bias)

    elif isinstance(m, nn.MultiheadAttention):
        nn.init.xavier_uniform_(m.in_proj_weight)
        if m.in_proj_bias is not None:
            nn.init.zeros_(m.in_proj_bias)
        if m.out_proj.weight is not None:
            nn.init.xavier_uniform_(m.out_proj.weight)
        if m.out_proj.bias is not None:
            nn.init.zeros_(m.out_proj.bias)

    elif hasattr(m, 'weight') and m.weight is not None:
        if m.weight.dim() > 1:
            nn.init.xavier_uniform_(m.weight)
        else:
            nn.init.zeros_(m.weight)
        if hasattr(m, 'bias') and m.bias is not None:
            nn.init.zeros_(m.bias)

"""## Their model but improved.

I improved their model by taking the good stuff from my baseline and it works well. I did some changes to the CausalConv1d module so you have to paste it as it is in a new file.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from einops.layers.torch import Rearrange
from tsl.nn.layers import GatedGraphNetwork, BatchNorm

class CausalConv1d(nn.Conv1d):
    """Causal convolution with padding=(kernel_size−1)*dilation on the left."""
    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):
        super().__init__(in_channels, out_channels,
                         kernel_size=kernel_size,
                         padding=0,
                         dilation=dilation,
                         **kwargs)
        self._padding = (kernel_size - 1) * dilation

    def forward(self, x):
        # pad only on the left
        x = F.pad(x, (self._padding, 0))
        return super().forward(x)


class EnhancedTCNGNN(nn.Module):
    """
    Enhanced TCN and GNN spatiotemporal backbone.

    This model begins by learning a distinct embedding for each weather station and for each
    forecast horizon, which are added to the initial input projections. By giving the network
    explicit station and time step embeddings it can learn location specific biases (e.g. alpine
    versus lowland) and dynamically adjust its treatment of near term versus long term lead
    times across the full 96 hour window.

    In each dilated TCN layer we follow the causal convolution with BatchNorm and a ReLU,
    then fuse its output via a residual connection. Between every TCN block and the GNN
    message passing we reshape via einops so the graph sees a clean [B, T, N, C] tensor.
    After stacking these TCN and GNN blocks we apply a temporal LayerNorm and full
    multi head self attention along the time axis. This combination stabilizes training,
    allows arbitrarily deep receptive fields, and via attention lets the model reweight
    past and future time steps beyond fixed dilation patterns.

    To prevent over reliance on deep representations we compute a learned sigmoid gate
    that interpolates between the self attended features and a linear skip projection of
    the raw inputs. Finally we reshape back to [B, T, N, C] and apply a second
    multi head attention across the station dimension, discovering non local spatial
    dependencies that a fixed distance based graph might miss.
    """
    def __init__(
        self,
        input_size: int,
        hidden_channels: int,
        output_dist: str,
        n_stations: int,
        num_layers: int = 4,
        kernel_size: int = 3,
        dropout_p: float = 0.2,
        causal_conv: bool = True,
        attn_heads: int = 4,
        station_heads: int = 2,
        max_lead: int = 96,
    ):
        super().__init__()
        # embeddings
        self.station_emb = nn.Embedding(n_stations, hidden_channels)
        self.horizon_emb = nn.Embedding(max_lead + 1, hidden_channels)

        # initial encoder
        self.input_proj = nn.Linear(input_size, hidden_channels)

        # einops helpers
        self.to_tcn    = Rearrange('b t n c -> (b n) c t', n=n_stations)
        self.from_tcn  = Rearrange('(b n) c t -> (b n) t c', n=n_stations)
        self.unflatten = Rearrange('(b n) t c -> b t n c', n=n_stations)

        self.tcn_layers  = nn.ModuleList()
        self.norm_layers = nn.ModuleList()
        self.gnn_layers  = nn.ModuleList()
        for l in range(num_layers):
            in_ch  = hidden_channels
            out_ch = hidden_channels
            conv = (CausalConv1d(in_ch, out_ch, kernel_size, dilation=2**l)
                    if causal_conv
                    else nn.Conv1d(in_ch, out_ch, kernel_size,
                                   padding=(kernel_size-1)//2,
                                   dilation=2**l))
            self.tcn_layers.append(conv)
            self.norm_layers.append(nn.BatchNorm1d(out_ch))
            self.gnn_layers.append(GatedGraphNetwork(input_size=out_ch,
                                                     output_size=out_ch))

        self.temporal_ln   = nn.LayerNorm(hidden_channels)
        self.temporal_attn = nn.MultiheadAttention(hidden_channels,
                                                   num_heads=attn_heads,
                                                   batch_first=True)

        self.gate      = nn.Linear(hidden_channels, hidden_channels)
        self.skip_proj = nn.Linear(input_size, hidden_channels)

        self.station_ln   = nn.LayerNorm(hidden_channels)
        self.station_attn = nn.MultiheadAttention(hidden_channels,
                                                  num_heads=station_heads,
                                                  batch_first=True)

        # output head
        self.output_distr = dist_to_layer[output_dist](input_size=hidden_channels)

    def forward(self, x, edge_index):
        B, T, N, C = x.shape

        h = self.to_tcn(x)

        ids      = repeat(torch.arange(N, device=x.device),
                          'n -> b n', b=B)
        ids      = rearrange(ids, 'b n -> (b n)')
        stat_emb = self.station_emb(ids)
        stat_emb = stat_emb.unsqueeze(-1).expand(-1, -1, T)


        h_proj = self.input_proj(rearrange(h, 'bn f t -> bn t f'))
        h = rearrange(h_proj, 'bn t h -> bn h t')
        h = h + stat_emb

        skips = []
        for tcn, bn, gnn in zip(self.tcn_layers,
                                self.norm_layers,
                                self.gnn_layers):
            # TCN
            res = h
            h   = tcn(h)
            h   = bn(h)
            h   = F.relu(h)
            h   = h + res
            skips.append(h)

            h_seq = self.unflatten(self.from_tcn(h))
            # GNN
            h_seq = gnn(h_seq, edge_index)
            h = self.to_tcn(h_seq)

        h = sum(skips)
        h = self.unflatten(self.from_tcn(h))

        h = rearrange(h, 'b t n h -> (b n) t h')
        h = self.temporal_ln(h)
        attn_out, _ = self.temporal_attn(h, h, h)
        h = h + attn_out

        raw  = rearrange(x, 'b t n f -> (b n) t f')
        skip = self.skip_proj(raw)
        gate = torch.sigmoid(self.gate(h))
        h    = gate * h + (1 - gate) * skip

        steps = repeat(torch.arange(T, device=x.device),
                       't -> (b n) t', b=B, n=N)
        h = h + self.horizon_emb(steps)

        h2 = rearrange(h, '(b n) t h -> (b t) n h', b=B, n=N)
        h2 = self.station_ln(h2)
        stat_out, _ = self.station_attn(h2, h2, h2)
        stat_out = rearrange(stat_out, '(b t) n h -> b t n h', b=B, t=T)

        h = rearrange(h, '(b n) t h -> b t n h', b=B, n=N)
        h = h + stat_out

        return self.output_distr(h)

"""## Anomalous Masking.

Has three types, naive, stationwise, stationwise log rets.
"""

tgt = xr.open_dataset("/content/drive/MyDrive/GDL/data/targets.nc")

# prompt: look at the 0.05, and 0.95 percentile value and print them for obs:wind_speed

percentiles = tgt['obs:wind_speed'].quantile([0.02, 0.99])
print(f"5th percentile: {percentiles.sel(quantile=0.02).values}")
print(f"95th percentile: {percentiles.sel(quantile=0.99).values}")

def compute_station_thresholds(dm, lower_q=0.05, upper_q=0.95):
    all_y = dm.train_dataset.target_data.reshape(-1, dm.train_dataset.stations)
    all_y = all_y[~np.isnan(all_y).any(axis=1)]
    mins = np.quantile(all_y, lower_q, axis=0)
    maxs = np.quantile(all_y, upper_q, axis=0)
    return mins, maxs

def mask_anomalous_per_station(y, mins, maxs):
    if y.dim()==4:
        y0 = y.squeeze(-1)
    else:
        y0 = y

    mins_t = torch.as_tensor(mins, device=y0.device)[None,None,:]
    maxs_t = torch.as_tensor(maxs, device=y0.device)[None,None,:]

    bad = (y0 < mins_t) | (y0 > maxs_t) | torch.isnan(y0)
    y_clean = y0.clone()
    y_clean[bad] = float('nan')

    return y_clean.unsqueeze(-1) if y.dim()==4 else y_clean

import torch
import numpy as np

def compute_return_thresholds(dm, lower_q=0.05, upper_q=0.95):
    """
    Compute per‐station quantile thresholds of the log‐returns
    Δℓ = log(yₜ / yₜ₋₁) over the training set.
    Returns two arrays of shape [N]: (min_return, max_return).
    """
    y = dm.train_dataset.target_data  # shape [T, L, N]
    # we only care about successive lead‐times: flatten over all ref‐times and leads>0
    # shift by one lead:
    y0 = y[:-1, :, :]   # at t−1
    y1 = y[1:, :, :]    # at t
    # avoid zeros / nans
    valid = np.isfinite(y0) & (y0 > 0) & np.isfinite(y1) & (y1 > 0)
    logret = np.zeros_like(y0)
    logret[valid] = np.log(y1[valid] / y0[valid])
    # now collapse all time‐axes:
    logret_flat = logret.reshape(-1, dm.train_dataset.stations)
    # keep only rows where every station is valid (or you can do per‐station mask individually)
    # here we compute per‐station quantiles directly
    mins = np.quantile(logret_flat, lower_q, axis=0)
    maxs = np.quantile(logret_flat, upper_q, axis=0)
    return mins, maxs

import torch
import torch.nn.functional as F

def mask_anomalous_by_return(y, min_ret, max_ret):
    """
    y: tensor [B,L,N] or [B,L,N,1]
    min_ret, max_ret: arrays of shape [N]
    Returns y with any (b,ℓ,n) masked if log(y[b,ℓ,n]/y[b,ℓ-1,n])
    ∉ [min_ret[n], max_ret[n]]. ℓ=0 is left as is (or NaN).
    """
    # squeeze trailing dim
    if y.dim()==4:
        y0 = y.squeeze(-1)  # [B,L,N]
    else:
        y0 = y             # [B,L,N]
    B,L,N = y0.shape

    # Prepare threshold tensors [1,1,N]
    min_t = torch.as_tensor(min_ret, device=y0.device)[None,None,:]
    max_t = torch.as_tensor(max_ret, device=y0.device)[None,None,:]

    # Compute returns only for ℓ=1..L-1
    prev = y0[:, :-1, :]      # [B, L-1, N]
    curr = y0[:,  1:, :]      # [B, L-1, N]

    # Safe log‐returns
    lr = torch.full_like(curr, float('nan'))
    valid = (prev > 0) & (curr > 0)
    lr[valid] = torch.log(curr[valid] / prev[valid])  # [B, L-1, N]

    # Now pad back to length L (NaN at ℓ=0)
    pad = torch.full((B,1,N), float('nan'), device=y0.device)
    lr_full = torch.cat([pad, lr], dim=1)  # [B, L, N]

    # Mask where return ∉ [min_t, max_t]
    bad = (lr_full < min_t) | (lr_full > max_t) | torch.isnan(lr_full)
    y_clean = y0.clone()
    y_clean[bad] = float('nan')

    return y_clean.unsqueeze(-1) if y.dim()==4 else y_clean

"""## Data Preparation

This is the standard stuff that can be found in there modules as well. But, thing to note here, there is a function that I defined here which is pretty important as it is used later on.

"""

#### DATA PREP
from tsl.ops.connectivity import adj_to_edge_index

nwp_model      = "ch2"
d_map          = {"ch2": 96}          # hours that correspond to each NWP model
hours_leadtime = d_map[nwp_model]

val_split            = 0.20
test_start_date      = "2024-05-16"
train_val_end_date   = "2023-09-30"
target_var           = "obs:wind_speed"

predictors = [
    f"{nwp_model}:wind_speed_ensavg",
    f"{nwp_model}:wind_speed_ensstd",
    f"{nwp_model}:mslp_difference_GVE_GUT_ensavg",
    f"{nwp_model}:mslp_difference_BAS_LUG_ensavg",
    "time:sin_hourofday",
    "time:cos_hourofday",
    "time:sin_dayofyear",
    "time:cos_dayofyear",
    "terrain:elevation_50m",
    "terrain:distance_to_alpine_ridge",
    "terrain:tpi_2000m",
    "terrain:std_2000m",
    "terrain:valley_norm_2000m",
    "terrain:sn_derivative_500m",
    "terrain:sn_derivative_2000m",
    "terrain:we_derivative_500m",
    "terrain:we_derivative_2000m",
    "terrain:sn_derivative_100000m",
]


# Common data module  MAKE SURE YOUR MODEL USES THAT
def load_data(knn=5, threshold=0.6, theta=None):
    import xarray as xr
    ds = xr.open_dataset("/content/drive/MyDrive/GDL/data/features.nc")   #TODO: Change Path
    ds_tgt = xr.open_dataset("/content/drive/MyDrive/GDL/data/targets.nc")    #TODO: Change Path
    dm = get_datamodule(
        ds=ds, ds_targets=ds_tgt,
        predictors=predictors,
        lead_time_hours=96,
        val_split=0.2,
        test_start_date="2024-05-16",
        train_val_end_date="2023-09-30",
        target_var="obs:wind_speed",
        return_graph=True, graph_kwargs={"knn":knn,"threshold":threshold,"theta":theta}
    )
    adj_tensor = torch.tensor(dm.adj_matrix, dtype=torch.float32)
    edge_index, _ = adj_to_edge_index(adj=adj_tensor)
    return dm, edge_index

"""## Log plots and Training Routine

This is probably one of the most important cells in the notebook. This contains the train script that should be used for every model that I made. This train script serves both for hyperparam tuning and training. We also employ early stopping.
"""

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import (
    OneCycleLR, CosineAnnealingLR,
    StepLR, ExponentialLR,
    CosineAnnealingWarmRestarts
)
from torch.utils.data import DataLoader
from tqdm.auto import tqdm


def log_prediction_plots(x, y, pred_dist, example_indices, stations, epoch, input_denormalizer, folder_name, model_name, filename, test=False):
    x = input_denormalizer(x) # bring inputs to their original range
    x = x.detach().cpu().numpy()
    y = y.detach().cpu().numpy()

    fig, axs = plt.subplots(2, 2, figsize=(15, 8))
    axs = axs.flatten()

    quantile_levels = torch.tensor([0.05, 0.25, 0.5, 0.75, 0.95]).repeat(*y.shape).to(pred_dist.mean.device)
    quantiles = pred_dist.icdf(quantile_levels).detach().cpu().numpy()

    time = time = np.arange(x.shape[1])

    for i, (b_idx, station) in enumerate(zip(example_indices, stations)):
        ax = axs[i]
        # First feature is expected to be the ensemble mean
        ax.plot(x[b_idx, :, station, 0], label='ens_mean', color='forestgreen')

        # Plot quantiles
        ax.fill_between(time, quantiles[b_idx,:, station,0], quantiles[b_idx,:, station,1],
                    alpha=0.15, color="blue", label="5%-95%")

        ax.fill_between(time, quantiles[b_idx,:, station,1], quantiles[b_idx,:, station,2],
                        alpha=0.35, color="blue", label="25%-75%")

        ax.plot(time, quantiles[b_idx, :, station,2], color="black", linestyle="--", label="Median (50%)")

        ax.fill_between(time, quantiles[b_idx,:, station,2], quantiles[b_idx,:, station,3],
                        alpha=0.35, color="blue")

        ax.fill_between(time, quantiles[b_idx,:, station,3], quantiles[b_idx,:, station,4],
                        alpha=0.15, color="blue")

        ax.plot(y[b_idx, :, station, 0], label='observed', color='mediumvioletred')
        ax.set_title(f'Station {station} at batch element {b_idx}')
        ax.set_xlabel("Lead time")
        ax.set_ylabel("Wind speed")

    axs[-1].legend() # only show legend in the last plot

    if test:
        plt.suptitle(f'Predictions at Test')
    else:
        plt.suptitle(f'Predictions at Epoch {epoch}')
    plt.tight_layout()


    # plot_filename = f"predictions_epoch_{epoch}.png"
    base_path = os.path.join(folder_name, model_name)
    os.makedirs(base_path, exist_ok=True)
    plot_filename = os.path.join(base_path, filename)
    plt.savefig(f"{plot_filename}.png")
    plt.close(fig)

def train_and_save(dm, edge_index, config, epochs=100, anomalous_mask=True, hyperparam_search=False, verbose=True, weight_initialization=True, device=torch.device('cpu')):
    """
    dm            : PostprocessDatamodule
    edge_index    : from adj_to_edge_index(dm.adj_matrix)
    config        : dict with keys:
      - "model": one of {"baseline","enhanced_tcn_gnn",}
      - hyperparams: learning rate, weight_decay, hidden_size, num_layers, dropout_p, scheduler type & params
    epochs        : how many epochs to train
    """

    model_name    = config["model"]
    lr            = config["lr"]
    wd            = config["weight_decay"]
    hs            = config["hidden_size"]
    nl            = config["num_layers"]
    dp            = config["dropout_p"]
    sch_name = config["scheduler"]
    if model_name != "baseline":
        kernel_size = config.get("kernel_size",3)

    # instantiate model
    if model_name == "baseline":
        model = EnhancedGRUBaseline(
            input_size = dm.train_dataset.features,
            hidden_channels = hs,
            output_dist = "LogNormal",
            n_stations = dm.train_dataset.stations,
            num_layers = nl,
            dropout_p = dp
        )
    elif model_name == "enhanced_tcn_gnn":
        model = EnhancedTCNGNN(input_size=dm.train_dataset.features,
                               hidden_channels=hs,
                               output_dist="LogNormal",
                               n_stations=dm.train_dataset.stations,
                               num_layers=nl,
                               kernel_size=kernel_size,
                               dropout_p=dp,)

    elif model_name == "tcn_gnn":
        model = TCN_GNN(num_layers=nl,
                        input_size=dm.train_dataset.features,
                        output_dist="LogNormal",
                        n_stations=dm.train_dataset.stations,
                        hidden_channels=hs,
                        dropout_p=dp,)
    else:
        raise ValueError(f"Unknown model {model_name}")

    model.to(device)
    if weight_initialization:
        model.apply(init_weights)
    edge_index = edge_index.to(device)
    mins, maxs = compute_return_thresholds(dm)
    # data loaders
    train_dl = DataLoader(dm.train_dataset, batch_size=config["batch_size"], shuffle=True,)
    val_dl   = DataLoader(dm.val_dataset,   batch_size=config["batch_size"], shuffle=False,)

    # optimizer + scheduler
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    if sch_name == "onecycle":
        scheduler = OneCycleLR(
            optimizer,
            max_lr=lr,
            epochs=epochs,
            steps_per_epoch=len(train_dl),
            pct_start=config.get("pct_start",0.3)
        )
    elif sch_name == "cosine":
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=config["T_max"]
        )
    elif sch_name == "steplr":
        scheduler = StepLR(
            optimizer,
            step_size=config["step_size"],
            gamma=config["gamma"]
        )
    elif sch_name == "exponential":
        scheduler = ExponentialLR(
            optimizer,
            gamma=config["exp_gamma"]
        )
    elif sch_name == "cosinewarm":
        scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=config["T_0"],
            T_mult=config.get("T_mult",1),
            eta_min=config.get("eta_min",1e-6)
        )
    else:
        raise ValueError(f"Unknown scheduler {sch_name}")

    # losses
    crps_crit = MaskedCRPSLogNormal()
    mae_crit  = MaskedL1Loss()

    horizons = [1,24,48,96]
    best_val_crps = float("inf")
    best_val_crps_h = {h: float("inf") for h in horizons}
    best_val_mae_h  = {h: float("inf") for h in horizons}
    best_state_dict = None
    early_stopping_counter = 0

    # output folders
    os.makedirs(f"/content/drive/MyDrive/GDL/models", exist_ok=True)    #TODO: Change Path
    os.makedirs(f"/content/drive/MyDrive/GDL/hyperparams", exist_ok=True)   #TODO: Change Path
    os.makedirs(f"/content/drive/MyDrive/GDL/plots/log_pred_plot_val/{model_name}", exist_ok=True)    #TODO: Change Path
    os.makedirs(f"/content/drive/MyDrive/GDL/plots/loss/{model_name}", exist_ok=True)   #TODO: Change Path
    avg_crps_loss_train = list()
    avg_mae_loss_train = list()
    avg_crps_loss_val = list()
    avg_mae_loss_val = list()

    for epoch in range(epochs):
        # TRAIN
        model.train()
        train_crps_sum = 0.0
        train_mae_sum  = 0.0
        torch.cuda.empty_cache()
        for i, (x,y) in enumerate(tqdm(train_dl, desc=f"Epoch {epoch} [Train]")):
            x,y = x.to(device), y.to(device)
            optimizer.zero_grad()
            # forward
            dist = model(x, edge_index) if model_name!="baseline" else model(x)
            if anomalous_mask:
                y = mask_anomalous_by_return(y, mins, maxs)

            loss = crps_crit(dist, y)
            mu   = dist.mean.squeeze(-1)
            mae  = mae_crit(mu, y.squeeze(-1))
            # backward
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(),1.0)
            optimizer.step()
            # scheduler step
            if sch_name=="cosinewarm":
                scheduler.step(epoch + i/len(train_dl))
            else:
                scheduler.step()

            train_crps_sum += loss.item()
            train_mae_sum  += mae.item()

        avg_train_crps = train_crps_sum / len(train_dl)
        avg_crps_loss_train.append(avg_train_crps)
        avg_train_mae  = train_mae_sum  / len(train_dl)
        avg_mae_loss_train.append(avg_train_mae)

        # VALIDATION
        model.eval()
        val_crps_sum = 0.0
        val_mae_sum  = 0.0
        sum_crps     = {h:0.0 for h in horizons}
        cnt_crps     = {h:0   for h in horizons}
        sum_mae      = {h:0.0 for h in horizons}
        cnt_mae      = {h:0   for h in horizons}

        with torch.no_grad():
            for x,y in tqdm(val_dl, desc=f"Epoch {epoch} [Valid]"):
                x,y = x.to(device), y.to(device)
                dist = model(x, edge_index) if model_name!="baseline" else model(x)
                if anomalous_mask:
                    y = mask_anomalous_by_return(y, mins, maxs)
                loss = crps_crit(dist, y)
                mu   = dist.mean.squeeze(-1)
                mae  = mae_crit(mu, y.squeeze(-1))
                val_crps_sum += loss.item()
                val_mae_sum  += mae.item()

                # per-horizon
                for h in horizons:
                    loc_h   = dist.loc[:,h,:,:].squeeze(-1)
                    sc_h    = dist.scale[:,h,:,:].squeeze(-1)
                    d_h     = torch.distributions.LogNormal(loc_h,sc_h)
                    y_h     = y[:,h,:,:].squeeze(-1)
                    valid   = (~torch.isnan(y_h)).sum().item()
                    c_h     = crps_crit(d_h, y_h).item()
                    m_h     = mae_crit(mu[:,h,:], y_h).item()
                    sum_crps[h] += c_h*valid
                    cnt_crps[h] += valid
                    sum_mae [h] += m_h*valid
                    cnt_mae [h] += valid

        avg_val_crps = val_crps_sum / len(val_dl)
        avg_crps_loss_val.append(avg_val_crps)
        avg_val_mae  = val_mae_sum  / len(val_dl)
        avg_mae_loss_val.append(avg_val_mae)
        avg_crps_h   = {h: sum_crps[h]/cnt_crps[h] for h in horizons}
        avg_mae_h    = {h: sum_mae [h]/cnt_mae [h] for h in horizons}

        # ——— LOG & PRINT —————————————————————————————————————
        if verbose and not hyperparam_search:
            print(f"\nEpoch {epoch+1}/{epochs}")
            print(f" Train →  CRPS: {avg_train_crps:.4f} │ MAE: {avg_train_mae:.4f}")
            print(f" Valid →  CRPS: {avg_val_crps:.4f} │ MAE: {avg_val_mae:.4f}\n")
            print(" Horizon │   CRPS   │    MAE ")
            print("-"*32)
            for h in horizons:
                print(f" {h:6d} │ {avg_crps_h[h]:8.4f} │ {avg_mae_h[h]:8.4f}")
            print()

        # ——— log_prediction_plots on last val batch ——————————————————
        if epoch == epochs - 1 and not hyperparam_search:
            log_prediction_plots(
                x = x.cpu(),
                y = y.cpu(),
                pred_dist = dist,
                example_indices = [0,0,0,0],
                stations        = [0,1,2,3],
                epoch           = epoch,
                input_denormalizer = dm.val_dataset.input_denormalizer,
                folder_name = f"/content/drive/MyDrive/GDL/plots/log_pred_plot_val/",     #TODO: Change Path or make it a parameter
                model_name = model_name,
                filename=f"epoch_{epoch+1}"
            )

        # ——— save best model + config —————————————————————————————————
        if avg_val_crps < best_val_crps:
            best_val_crps = avg_val_crps
            best_val_crps_h = avg_crps_h
            best_val_mae_h  = avg_mae_h
            best_state_dict = model.state_dict()
            early_stopping_counter = 0

        if early_stopping_counter == 10:
            print(f"Early stopping triggered at epoch {epoch+1}")
            break
        else:
            early_stopping_counter += 1

    if verbose and not hyperparam_search:
        print("Best validation CRPS:", best_val_crps)
    return best_state_dict, best_val_crps, best_val_crps_h, best_val_mae_h, avg_crps_loss_train, avg_crps_loss_val

"""## Hyper parameter tuning

This is where we initialize the search space and randomly sample from. And then tune our hyperparams for each model

"""

import os, json
import torch
import optuna

def get_search_space(trial):
    space = {
        "hidden_size":   trial.suggest_categorical("hidden_size",   [32, 48, 64]),
        "num_layers":    trial.suggest_int(        "num_layers",    4, 5),
        "dropout_p":     trial.suggest_float(      "dropout_p",     0.1, 0.3),
        "lr":            trial.suggest_float( "lr",            1e-4, 1e-2),
        "weight_decay":  trial.suggest_float( "weight_decay",  1e-7, 1e-5),
        "batch_size":    trial.suggest_categorical("batch_size",   [32]),
        "scheduler":     trial.suggest_categorical("scheduler",    [
                             "onecycle","cosine","steplr","exponential","cosinewarm"
                         ]),
    }

    sch = space["scheduler"]
    if sch == "onecycle":
        space["pct_start"]  = trial.suggest_float("pct_start", 0.1, 0.4)
    elif sch == "cosine":
        space["T_max"]      = trial.suggest_int("T_max",  10, 100)
    elif sch == "steplr":
        space["step_size"]  = trial.suggest_int("step_size", 5, 50)
        space["gamma"]      = trial.suggest_float("gamma",     0.1, 0.9)
    elif sch == "exponential":
        space["exp_gamma"]  = trial.suggest_float("exp_gamma",  0.80, 0.90)
    elif sch == "cosinewarm":
        space["T_0"]        = trial.suggest_int("T_0",   5, 20)
        space["T_mult"]     = trial.suggest_int("T_mult", 1, 4)
        space["eta_min"]    = trial.suggest_loguniform("eta_min", 1e-7, 1e-5)
    return space

def make_objective(model_name, masked_anom):
    def objective(trial):
        # clear GPU mem
        torch.cuda.empty_cache()

        cfg = get_search_space(trial)
        cfg["model"] = model_name

        if model_name in ("enhanced_tcn_gnn"):
            cfg["kernel_size"] = trial.suggest_int("kernel_size", 3, 5)
            cfg['knn'] = trial.suggest_categorical("knn", [5, 10, 15])
            cfg['threshold'] = trial.suggest_float("threshold", 0.5, 0.9)
            cfg['theta'] = trial.suggest_categorical("theta", ["std", "median", "factormedian"])
            dm, edge_index = load_data(knn=cfg["knn"], threshold=cfg["threshold"], theta=cfg["theta"])
        else:
            dm, edge_index = load_data()


        # train for 50 epochs to speed up search
        _, val_crps, _, _, _, _ = train_and_save(
            dm, edge_index, cfg,
            epochs=50,
            anomalous_mask=masked_anom,
            hyperparam_search=True,
            verbose=False,
            device=torch.device("cuda")
        )
        return val_crps
    return objective

def run_hyperopt_all(out_folder, n_trials=20):
    os.makedirs(out_folder, exist_ok=True)
    for model_name in ["baseline", "enhanced_tcn_gnn"]:   # We train for both models
        for masked_anomalous in [False, True]:    # Also for both masks
            print(f"Optimizing {model_name} with anomalous mask = {masked_anomalous}")
            print()
            study = optuna.create_study(direction="minimize", study_name=model_name)
            study.optimize(make_objective(model_name, masked_anomalous), n_trials=n_trials)

            best = study.best_params
            best["model"] = model_name

            if masked_anomalous:
                path = os.path.join(out_folder, f"{model_name}_best_masked.json")   # Save it to the out folder with this name
            else:
                path = os.path.join(out_folder, f"{model_name}_best.json")
            with open(path,"w") as f:
                json.dump(best, f, indent=2)
            print(f"{model_name} best CRPS = {study.best_value:.4f}, saved to {path}")
            print()

# hyperparam_base = "/content/drive/MyDrive/GDL/hyperparams_new"    #TODO: Change Path
# run_hyperopt_all(hyperparam_base, n_trials=100)

"""## Initialization toggle training

Now we will train the model with the model.init function off or on to see the changes it might bring to the convergence, we will also save the average losses for both and plot.
"""

dm, edge_index = load_data()

mins, maxs = compute_station_thresholds(dm)
old_min, old_max = 0.2, 10.

import random
import numpy as np
import matplotlib.pyplot as plt

# Extract raw training targets: shape [T, L, N, 1]
y = dm.train_dataset.target_data  # numpy array
y = y.squeeze(-1)                 # shape [T, L, N]
T, L, S = y.shape

# Flatten over time and lead: shape [T*L, N]
y_flat = y.reshape(-1, S)

# Randomly pick 5 stations
stations = random.sample(range(S), 5)

# Plot each station's distribution with its min/max thresholds
for s in stations:
    plt.figure(figsize=(6, 4))
    plt.hist(y_flat[:, s], bins=50, alpha=0.5)
    plt.axvline(mins[s], linestyle='--', label='Station Wise Mask', color='b')
    plt.axvline(maxs[s], linestyle='-.', color='b')
    plt.axvline(old_min, linestyle='--', label='Naive Mask', color='r')
    plt.axvline(old_max, linestyle='-.', color='r')
    plt.title(f'Station {s}: Wind Speed Distribution')
    plt.xlabel(r'Wind Speed $\frac{m}{s}$')
    plt.ylabel('Frequency')
    plt.legend()
    plt.tight_layout()
    plt.show()

import random


loss_without_weight_init = list()
loss_with_weight_init = list()


HYPERPARAM_DIR = "/content/drive/MyDrive/GDL/hyperparams_new"   #TODO: Change Path
OUTPUT_DIR     = "/content/drive/MyDrive/GDL/models_final"    #TODO: Change Path
METRICS_LOG    = "/content/drive/MyDrive/GDL/final_metrics.json"    #TODO: Change Path
os.makedirs(OUTPUT_DIR, exist_ok=True)
models = ["enhanced_tcn_gnn"]#, "tcn_gnn"]

all_results = {}

for model_name in models:
    cfg_masked_path = os.path.join(HYPERPARAM_DIR, f"enhanced_tcn_gnn_best_masked.json")
    cfg_unmasked_path = os.path.join(HYPERPARAM_DIR, f"enhanced_tcn_gnn_best.json")

    best_cfg_unmasked = json.load(open(cfg_unmasked_path))
    best_cfg_masked   = json.load(open(cfg_masked_path))

    all_results[model_name] = {}

    for amomalous_mask, cfg in [(True, best_cfg_masked), (False, best_cfg_unmasked)]:

        mode = "masked" if amomalous_mask else "unmasked"
        all_results[model_name][mode] = {}
        # load data once per config
        if model_name == "baseline":
            dm, edge_index = load_data()
        else:
            # enhanced_tcn_gnn needs graph kwargs from config
            dm, edge_index = load_data(
                knn       = cfg["knn"],
                threshold = cfg["threshold"],
                theta     = cfg["theta"],
            )

        torch.cuda.empty_cache()
        random.seed(42)
        np.random.seed(42)
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(42)

        print(f"Training {model_name} | Mask={mode} | seed={42}")

        state_dict, best_crps, best_crps_h, best_mae_h, train_loss_list, val_loss_list = train_and_save(
            dm,
            edge_index,
            {**cfg, "model":model_name},
            epochs=50,
            anomalous_mask=amomalous_mask,
            hyperparam_search=False,
            verbose=True,
            device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        )

        # record in summary dict
        all_results[model_name][mode]['val_loss'] = val_loss_list
        all_results[model_name][mode]['train_loss'] = train_loss_list

# prompt: Plot val loss difference between enhanced_tcn_gnn between masked and unmasked

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))

# Make sure that the size of the list is same, else make it same
if len(all_results['enhanced_tcn_gnn']['masked']['val_loss']) != len(all_results['enhanced_tcn_gnn']['unmasked']['val_loss']):
    min_length = min(len(all_results['enhanced_tcn_gnn']['masked']['val_loss']), len(all_results['enhanced_tcn_gnn']['unmasked']['val_loss']))
    all_results['enhanced_tcn_gnn']['masked']['val_loss'] = all_results['enhanced_tcn_gnn']['masked']['val_loss'][:min_length]
    all_results['enhanced_tcn_gnn']['unmasked']['val_loss'] = all_results['enhanced_tcn_gnn']['unmasked']['val_loss'][:min_length]


plt.plot(all_results['enhanced_tcn_gnn']['masked']['val_loss'], label='Masked Val Loss')
plt.plot(all_results['enhanced_tcn_gnn']['unmasked']['val_loss'], label='Unmasked Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss (CRPS)')
plt.title('Validation Loss Comparison: Enhanced TCN-GNN (Masked vs Unmasked)')
plt.legend()
plt.grid(True)

# Define the output directory for plots
plot_output_dir = "/content/drive/MyDrive/GDL/plots/loss/enhanced_tcn_gnn" # TODO: Change Path or make it a parameter
os.makedirs(plot_output_dir, exist_ok=True)
plot_filename = os.path.join(plot_output_dir, "enhanced_tcn_gnn_val_loss_comparison.png")
plt.savefig(plot_filename)
plt.show()

# prompt: Plot val loss difference between tcn_gnn and enhanced_tcn_gnn

import matplotlib.pyplot as plt
# Assuming the training and validation loss lists are already populated from the training loop.
# e.g., train_loss_tcn_gnn, val_loss_tcn_gnn, train_loss_enhanced_tcn_gnn, val_loss_enhanced_tcn_gnn
# These lists are available in the `all_results` dictionary from the previous cell.

# Example access (adjust variable names if needed based on how you stored the results)
val_loss_tcn_gnn = all_results['tcn_gnn']['val_loss']
val_loss_enhanced_tcn_gnn = all_results['enhanced_tcn_gnn']['val_loss']

# Make sure that the size of the list is same, else make it same
if len(val_loss_tcn_gnn) != len(val_loss_enhanced_tcn_gnn):
    min_length = min(len(val_loss_tcn_gnn), len(val_loss_enhanced_tcn_gnn))
    val_loss_tcn_gnn = val_loss_tcn_gnn[:min_length]
    val_loss_enhanced_tcn_gnn = val_loss_enhanced_tcn_gnn[:min_length]

plt.figure(figsize=(10, 6))
plt.plot(val_loss_tcn_gnn, label='TCN-GNN Val Loss')
plt.plot(val_loss_enhanced_tcn_gnn, label='Enhanced TCN-GNN Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss (CRPS)')
plt.title('Validation Loss Comparison: TCN-GNN vs Enhanced TCN-GNN')
plt.legend()
plt.grid(True)

# Save the plot
plot_filename = os.path.join(f"/content/drive/MyDrive/GDL/plots/loss/", "val_loss_comparison_gnns.png") #TODO: Change Path or make it a parameter
plt.savefig(plot_filename)
plt.show()

# prompt: Plot Val loss between woi and wi from all_results dict

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))

# First check if the val_loss has the same length, if not, make them same
if len(all_results['baseline']['woi']['val_loss']) != len(all_results['baseline']['wi']['val_loss']):
    min_length = min(len(all_results['baseline']['woi']['val_loss']), len(all_results['baseline']['wi']['val_loss']))
    all_results['baseline']['woi']['val_loss'] = all_results['baseline']['woi']['val_loss'][:min_length]
    all_results['baseline']['wi']['val_loss'] = all_results['baseline']['wi']['val_loss'][:min_length]
    # all_results['baseline']['woi']['train_loss'] = all_results['baseline']['woi']['train_loss'][:min_length]
    # all_results['baseline']['wi']['train_loss'] = all_results['baseline']['wi']['train_loss'][:min_length]


if 'enhanced_tcn_gnn' in all_results:
    plt.plot(all_results['enhanced_tcn_gnn']['woi']['val_loss'], label='EnhancedTCNGNN without Weight Initialization')
    plt.plot(all_results['enhanced_tcn_gnn']['wi']['val_loss'], label='EnhancedTCNGNN with Weight Initialization')
else:
    plt.plot(all_results['baseline']['woi']['val_loss'], label='Baseline without Weight Initialization')
    plt.plot(all_results['baseline']['wi']['val_loss'], label='Baseline with Weight Initialization')


plt.title('Validation Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Validation CRPS Loss')
plt.legend()
plt.grid(True)

plot_folder = "/content/drive/MyDrive/GDL/plots/loss/val_loss_comparison"
os.makedirs(plot_folder, exist_ok=True)
plt.savefig(os.path.join(plot_folder, 'val_loss_comparison.png'))
plt.show()

"""## 5 seed training

This is the script where we have a best config for the models, and then we just run 5 seeds over it and save the models where we want to.
"""

import random

# where your tuned JSONs live
HYPERPARAM_DIR = "/content/drive/MyDrive/GDL/hyperparams_new"   #TODO: Change Path
OUTPUT_DIR     = "/content/drive/MyDrive/GDL/models_final"    #TODO: Change Path
METRICS_LOG    = "/content/drive/MyDrive/GDL/final_metrics_log_ret.json"    #TODO: Change Path
os.makedirs(OUTPUT_DIR, exist_ok=True)

models = ["baseline", "enhanced_tcn_gnn"]
seeds  = [0, 1, 2, 3, 4]

all_results = {}

for model_name in models:
    # cfg_unmasked_path = os.path.join(HYPERPARAM_DIR, f"{model_name}_best.json")
    cfg_masked_path   = os.path.join(HYPERPARAM_DIR, f"{model_name}_best_masked.json")

    # best_cfg_unmasked = json.load(open(cfg_unmasked_path))
    best_cfg_masked   = json.load(open(cfg_masked_path))

    all_results[model_name] = {"unmasked":{}, "masked":{}}

    # we’ll do two runs: anomalous_mask=False (plain) and True (masked)
    for anomalous_mask, best_cfg in [(True,  best_cfg_masked)]:
                                    #  (False, best_cfg_unmasked)]:

        mode = "masked" if anomalous_mask else "unmasked"

        # load data once per config
        if model_name == "baseline":
            dm, edge_index = load_data()
        else:
            # enhanced_tcn_gnn needs graph kwargs from config
            dm, edge_index = load_data(
                knn       = best_cfg["knn"],
                threshold = best_cfg["threshold"],
                theta     = best_cfg["theta"],
            )

        for seed in seeds:
            torch.cuda.empty_cache()
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)

            print(f"Training {model_name} | mode={mode} | seed={seed}")

            state_dict, best_crps, best_crps_h, best_mae_h, _, _ = train_and_save(
                dm,
                edge_index,
                {**best_cfg, "model":model_name},
                epochs=100,
                anomalous_mask=anomalous_mask,
                hyperparam_search=False,
                verbose=True,
                device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
            )

            # saving checkpoint
            ckpt_name = f"{model_name}_{mode}_seed{seed}_log_ret.pth"
            ckpt_path = os.path.join(OUTPUT_DIR, ckpt_name)
            torch.save(state_dict, ckpt_path)

            # record in summary dict
            all_results[model_name][mode][str(seed)] = {
                "best_crps":            best_crps,
                "best_crps_by_horizon": best_crps_h,
                "best_mae_by_horizon":  best_mae_h,
                "checkpoint":           ckpt_path,
            }

with open(METRICS_LOG, "w") as f:
    json.dump(all_results, f, indent=2)

print(f"Checkpoints in {OUTPUT_DIR}, summary -> {METRICS_LOG}")

"""## Adapted Leon's Script

This is the script that saves the mu and log var for the log normal dist over test set.
"""

import os
import json
import torch
import numpy as np
from torch.utils.data import DataLoader

DIST_OUTPUT_DIR     = "/content/drive/MyDrive/GDL/models_dist"    #TODO: Change Path
os.makedirs(DIST_OUTPUT_DIR, exist_ok=True)

def load_model(checkpoint_path, device, model_name, model_kwargs):
    if model_name == "baseline":
        mdl = EnhancedGRUBaseline(**model_kwargs)
    else:
        mdl = EnhancedTCNGNN(**model_kwargs)
    mdl.load_state_dict(torch.load(checkpoint_path, map_location=device))
    return mdl.to(device).eval()

def test_model(model, loader, device, model_name, edge_index=None):
    means, scales = [], []
    with torch.no_grad():
        for x_batch, _ in loader:
            x_batch = x_batch.to(device)
            if model_name == "baseline":
                dist = model(x_batch)
            else:
                edge_index = edge_index.to(device)
                dist = model(x_batch, edge_index)
            # dist.loc, dist.scale both shape [B, L, N, 1]
            means .append(dist.loc  .cpu().numpy())
            scales.append(dist.scale.cpu().numpy())
    return np.concatenate(means, axis=0), np.concatenate(scales, axis=0)

with open("/content/drive/MyDrive/GDL/final_metrics_log_ret.json") as f:    #TODO: Change Path -> Where you read the final metrics from
    all_results = json.load(f)

os.makedirs("outputs", exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for model_name, modes in all_results.items():
    for mode, seeds_dict in modes.items():
        for seed, info in seeds_dict.items():
            ckpt_path = info["checkpoint"]
            run_name  = f"{model_name}_{mode}_seed{seed}_log_ret"

            if model_name == "baseline":
                dm, edge_index = load_data()
                cfg_path = os.path.join(
                    "/content/drive/MyDrive/GDL/hyperparams_new",   #TODO: Change Path
                    f"{model_name}_{'best_masked' if mode=='masked' else 'best'}.json"
                )
                cfg = json.load(open(cfg_path))
            else:
                cfg_path = os.path.join(
                    "/content/drive/MyDrive/GDL/hyperparams_new",   #TODO: Change Path
                    f"{model_name}_{'best_masked' if mode=='masked' else 'best'}.json"
                )
                cfg = json.load(open(cfg_path))
                dm, edge_index = load_data(
                    knn       = cfg["knn"],
                    threshold = cfg["threshold"],
                    theta     = cfg["theta"]
                )

            batch_size = 32 if model_name=="baseline" else cfg.get("batch_size",32)
            test_loader = DataLoader(dm.test_dataset, batch_size=batch_size, shuffle=False)

            model_kwargs = dict(
                input_size    = dm.train_dataset.features,
                hidden_channels = cfg["hidden_size"],
                output_dist     = "LogNormal",
                n_stations      = dm.train_dataset.stations,
                num_layers      = cfg["num_layers"],
                dropout_p       = cfg["dropout_p"],
            )
            if model_name != "baseline":
                model_kwargs["kernel_size"] = cfg.get("kernel_size",3)

            model = load_model(ckpt_path, device, model_name, model_kwargs)
            means_arr, scales_arr = test_model(model, test_loader, device, model_name, edge_index)

            out_path = os.path.join(DIST_OUTPUT_DIR, f"{run_name}_test_outputs_lr.npz")
            np.savez_compressed(out_path, mean=means_arr, std=scales_arr)
            print(f"saved {out_path}")

"""#Plotting Routine and utils- Skip to the next area where you find the heading

Right now I have to fix this a bit. We can do this later since this is just the plotting routine.
"""

import torch
import numpy as np
from tqdm.auto import tqdm

def evaluate_and_plot(
    model,
    dataloader,
    edge_index,
    model_name: str,
    device: torch.device,
    mask_anomalous: bool = True,
    out_plot_folder: str = "/content/drive/MyDrive/GDL/plots/log_pred_plot_test",
    filename=None
):
    """
    Runs the model on `dataloader`, prints per‐horizon CRPS/MAE,
    and saves one log‐prediction plot for station 0.
    """
    model.eval()
    edge_index = edge_index.to(device)
    crps_crit = MaskedCRPSLogNormal()
    mae_crit  = MaskedL1Loss()

    horizons = [1,24,48,96]
    sum_crps   = {h: 0.0 for h in horizons}
    cnt_crps   = {h: 0   for h in horizons}
    sum_mae    = {h: 0.0 for h in horizons}
    cnt_mae    = {h: 0   for h in horizons}

    # for the final plot
    last_x, last_y, last_dist = None, None, None

    with torch.no_grad():
        for x, y in tqdm(dataloader, desc="Test"):
            x = x.to(device)           # [B, L, N, P]
            y = y.to(device)           # [B, L, N, 1]

            # forward
            if model_name == "baseline":
                dist = model(x)
            else:
                dist = model(x, edge_index)

            if mask_anomalous:
                y = mask_anomalous_per_station(y, mins, maxs)

            # compute per‐horizon
            mu    = dist.mean.squeeze(-1)   # [B, L, N]
            truth = y.squeeze(-1)           # [B, L, N]

            for h in horizons:
                # CRPS at lead‐h
                loc_h   = dist.loc[:, h, :, :].squeeze(-1)
                sc_h    = dist.scale[:, h, :, :].squeeze(-1)
                d_h     = torch.distributions.LogNormal(loc_h, sc_h)
                y_h     = truth[:, h, :]

                valid  = (~torch.isnan(y_h)).sum().item()
                c_h    = crps_crit(d_h, y_h).item()
                m_h    = mae_crit(mu[:,h,:], y_h).item()

                sum_crps[h] += c_h * valid
                cnt_crps[h] += valid
                sum_mae[h]  += m_h * valid
                cnt_mae[h]  += valid

            # stash for plot
            last_x, last_y, last_dist = x.cpu(), y.cpu(), dist

    # compute averages
    avg_crps = {h: sum_crps[h]/cnt_crps[h] for h in horizons}
    avg_mae  = {h:  sum_mae[h]/cnt_mae[h]  for h in horizons}

    # print
    print("\nTest set performance:")
    print(" Horizon │   CRPS   │    MAE ")
    print("-"*32)
    for h in horizons:
        print(f" {h:6d} │ {avg_crps[h]:8.4f} │ {avg_mae[h]:8.4f}")
    print()

    # now one log‐prediction plot for station 0
    os.makedirs(out_plot_folder, exist_ok=True)
    log_prediction_plots(
        x = last_x,                      # [B,L,N,P]
        y = last_y,                      # [B,L,N,1]
        pred_dist = last_dist,           # Distribution over [B,L,N]
        example_indices = [1, 1, 1, 1],           # pick batch‐element 0
        stations        = [100, 101, 102, 103],           # just station 0
        epoch           = "test",
        input_denormalizer = dm.test_dataset.input_denormalizer,
        folder_name = out_plot_folder,
        model_name  = model_name,
        filename    = filename,
        test=True
    )
    print(f"Saved test log‐prediction plot for station 0 to {out_plot_folder}/{model_name}_station0.png")

with open("/content/drive/MyDrive/GDL/final_metrics.json") as f:    #TODO: Change Path -> Where you read the final metrics from
    all_results = json.load(f)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for model_name, modes in all_results.items():
    for mode, seeds_dict in modes.items():
        for seed, info in seeds_dict.items():
            ckpt_path = info["checkpoint"]
            run_name  = f"{model_name}_{mode}_seed{seed}"

            if model_name == "baseline":
                dm, edge_index = load_data()
                cfg_path = os.path.join(
                    "/content/drive/MyDrive/GDL/hyperparams_new",   #TODO: Change Path
                    f"{model_name}_{'best_masked' if mode=='masked' else 'best'}.json"
                )
                cfg = json.load(open(cfg_path))
            else:
                cfg_path = os.path.join(
                    "/content/drive/MyDrive/GDL/hyperparams_new",   #TODO: Change Path
                    f"{model_name}_{'best_masked' if mode=='masked' else 'best'}.json"
                )
                cfg = json.load(open(cfg_path))
                dm, edge_index = load_data(
                    knn       = cfg["knn"],
                    threshold = cfg["threshold"],
                    theta     = cfg["theta"]
                )

            batch_size = 32 if model_name=="baseline" else cfg.get("batch_size",32)
            test_loader = DataLoader(dm.test_dataset, batch_size=batch_size, shuffle=False)

            model_kwargs = dict(
                input_size    = dm.train_dataset.features,
                hidden_channels = cfg["hidden_size"],
                output_dist     = "LogNormal",
                n_stations      = dm.train_dataset.stations,
                num_layers      = cfg["num_layers"],
                dropout_p       = cfg["dropout_p"],
            )
            if model_name != "baseline":
                model_kwargs["kernel_size"] = cfg.get("kernel_size",3)

            model = load_model(ckpt_path, device, model_name, model_kwargs)
            evaluate_and_plot(model, test_loader, edge_index, model_name, device, mask_anomalous=mode, filename=f"{model_name}_{mode}_{seed}")
            # print(f"saved {out_path}")

import os
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

def plot_rank_histogram(
    model,
    dataloader: DataLoader,
    edge_index,
    model_name: str,
    device,
    n_samples: int = 20,
    horizons: list = [1,24,48,96],
    anomalous_mask = False,
    out_dir: str = "/content/drive/MyDrive/GDL/plots/calibration",   #TODO: Change Path
    filename=None
):
    device = next(model.parameters()).device
    edge_index = edge_index.to(device)
    model.eval()

    # collect ranks per horizon
    ranks = {h: [] for h in horizons}

    with torch.no_grad():
        for x, y in dataloader:
            x = x.to(device)
            y = y.to(device).squeeze(-1)
            # mask anomalies exactly as during training
            if anomalous_mask:
                y = mask_anomalous_per_station(y, mins, maxs)
            # get predictive distribution
            dist = model(x) if model_name=="baseline" else model(x, edge_index)
            # sample trajectories: [n_samples, B, L, N]
            samp = dist.rsample((n_samples,)).squeeze(-1).cpu().numpy()
            truth = y.cpu().numpy()  # [B, L, N]

            for h in horizons:
                s_h = samp[:, :, h, :]   # [n_samples, B, N]
                t_h = truth[:, h, :]     # [B, N]
                # count how many samples < truth → rank in [0…n_samples]
                ranks_h = np.sum(s_h < t_h[None,:,:], axis=0)
                ranks[h].extend(ranks_h.flatten().tolist())

    # plot
    if not anomalous_mask:
        calib_base = os.path.join(out_dir, model_name, "unmasked")
    else:
        calib_base = os.path.join(out_dir, model_name, "masked")
    os.makedirs(calib_base, exist_ok=True)

    fig, axs = plt.subplots(2,2,figsize=(12,8))
    axs = axs.flatten()
    bins = np.arange(n_samples+2) - 0.5

    for i, h in enumerate(horizons):
        axs[i].hist(ranks[h], bins=bins, edgecolor="k")
        axs[i].set_title(f"Rank histogram — lead {h}h")
        axs[i].set_xlim(-0.5, n_samples+0.5)
        axs[i].set_ylabel("Frequency")

    plt.tight_layout()
    outpath = os.path.join(calib_base, f"{'rankhist_all.png' if not filename else filename}")
    fig.savefig(outpath)
    plt.close(fig)
    print(f"▶ Saved rank histograms to {outpath}")

# — after you’ve loaded `model`, moved it to `device`, and built `test_loader` —
# plot_rank_histogram(
#     model       = model,
#     dataloader  = test_loader,
#     dm          = dm,
#     model_name  = "baseline_test",     # or "enhanced_tcn_gnn_test"
#     n_samples   = 20,
#     horizons    = [1,24,48,96],
#     out_dir     = "/content/drive/MyDrive/GDL/plots/calibration"    #TODO: Change Path
# )

with open("/content/drive/MyDrive/GDL/final_metrics.json") as f:    #TODO: Change Path -> Where you read the final metrics from
    all_results = json.load(f)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for model_name, modes in all_results.items():
    for mode, seeds_dict in modes.items():
        for seed, info in seeds_dict.items():
            ckpt_path = info["checkpoint"]
            run_name  = f"{model_name}_{mode}_seed{seed}"

            if model_name == "baseline":
                dm, edge_index = load_data()
                cfg_path = os.path.join(
                    "/content/drive/MyDrive/GDL/hyperparams_new",   #TODO: Change Path
                    f"{model_name}_{'best_masked' if mode=='masked' else 'best'}.json"
                )
                cfg = json.load(open(cfg_path))
            else:
                cfg_path = os.path.join(
                    "/content/drive/MyDrive/GDL/hyperparams_new",   #TODO: Change Path
                    f"{model_name}_{'best_masked' if mode=='masked' else 'best'}.json"
                )
                cfg = json.load(open(cfg_path))
                dm, edge_index = load_data(
                    knn       = cfg["knn"],
                    threshold = cfg["threshold"],
                    theta     = cfg["theta"]
                )

            batch_size = 32 if model_name=="baseline" else cfg.get("batch_size",32)
            test_loader = DataLoader(dm.test_dataset, batch_size=batch_size, shuffle=False)

            model_kwargs = dict(
                input_size    = dm.train_dataset.features,
                hidden_channels = cfg["hidden_size"],
                output_dist     = "LogNormal",
                n_stations      = dm.train_dataset.stations,
                num_layers      = cfg["num_layers"],
                dropout_p       = cfg["dropout_p"],
            )
            if model_name != "baseline":
                model_kwargs["kernel_size"] = cfg.get("kernel_size",3)

            model = load_model(ckpt_path, device, model_name, model_kwargs)
            plot_rank_histogram(model, test_loader, edge_index, model_name, device, anomalous_mask=mode, filename=f"rankall_hist_{seed}")

"""# Plot and utils over - Skip over

Don't skip the cells below. They evaluate the model and then gives avg crps and mae score over baseline and enhanced_tcn_gnn
"""

import os
import json
import torch
import numpy as np
from torch.utils.data import DataLoader
import statistics


# Paths
HYPERPARAM_DIR          = "/content/drive/MyDrive/GDL/hyperparams_new"
METRICS_LOG_BASELINE    = "/content/drive/MyDrive/GDL/final_metrics.json"
METRICS_LOG_TCN         = "/content/drive/MyDrive/GDL/final_metrics.json"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# horizons we care about
horizons = [1, 24, 48, 96]

def evaluate_checkpoint(model, dm, edge_index, checkpoint_path, mode):
    """
    Load a state_dict into model, evaluate on dm.test_dataset,
    return per-horizon (crps, mae) tuple of dicts.
    """
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    edge_index = edge_index.to(device)
    model.to(device).eval()

    crps_crit = MaskedCRPSLogNormal()
    mae_crit  = MaskedL1Loss()

    sum_crps = {h:0.0 for h in horizons}
    cnt_crps = {h:0   for h in horizons}
    sum_mae  = {h:0.0 for h in horizons}
    cnt_mae  = {h:0   for h in horizons}

    loader = DataLoader(dm.test_dataset, batch_size=32, shuffle=False, num_workers=4)
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            y = y.to(device)  # shape [B, L, N, 1]
            if mode == "masked":
                y = mask_anomalous_per_station(y, mins, maxs)

            # forward
            if isinstance(model, EnhancedGRUBaseline):
                dist = model(x)
            else:
                # EnhancedTCNGNN expects edge_index
                dist = model(x, edge_index)

            mu    = dist.mean.squeeze(-1)    # [B, L, N]
            truth = y.squeeze(-1)            # [B, L, N]

            for h in horizons:
                loc_h = dist.loc[:,h,:,:].squeeze(-1)
                sc_h  = dist.scale[:,h,:,:].squeeze(-1)
                d_h   = torch.distributions.LogNormal(loc_h, sc_h)

                y_h = truth[:,h,:]
                valid = (~torch.isnan(y_h)).sum().item()
                if valid==0:
                    continue

                sum_crps[h] += crps_crit(d_h, y_h).item() * valid
                cnt_crps[h] += valid

                sum_mae[h]  += mae_crit(mu[:,h,:], y_h).item() * valid
                cnt_mae[h]  += valid

    # compute averages
    avg_crps = {h: sum_crps[h]/cnt_crps[h] for h in horizons}
    avg_mae  = {h: sum_mae [h]/cnt_mae [h]  for h in horizons}
    return avg_crps, avg_mae

# Loop over both models
for model_name, metrics_path in [
    ("baseline", METRICS_LOG_BASELINE),
    ("enhanced_tcn_gnn", METRICS_LOG_TCN),
]:
    print(f"\n=== Model: {model_name} ===")
    results = json.load(open(metrics_path))

    for mode in ("unmasked","masked"):
        if mode not in results[model_name] or not results[model_name][mode]:
            continue

        # load the best hyperparams (to rebuild the model & load data)
        cfg_path = os.path.join(HYPERPARAM_DIR,
                                f"{model_name}_best" +
                                ("_masked.json" if mode=="masked" else ".json"))
        best_cfg = json.load(open(cfg_path))

        # build dm & edge_index once per mode
        if model_name == "baseline":
            dm, edge_index = load_data()
        else:
            dm, edge_index = load_data(
                knn       = best_cfg["knn"],
                threshold = best_cfg["threshold"],
                theta     = best_cfg["theta"],
            )

        # accumulate over seeds
        scores_crps = {h: [] for h in horizons}
        scores_mae  = {h: [] for h in horizons}
        n_seeds = len(results[model_name][mode])

        for seed, info in results[model_name][mode].items():
            ckpt = info["checkpoint"]
            # build model as before…
            if model_name == "baseline":
                model = EnhancedGRUBaseline(
                    input_size      = dm.train_dataset.features,
                    hidden_channels = best_cfg["hidden_size"],
                    output_dist     = "LogNormal",
                    n_stations      = dm.train_dataset.stations,
                    num_layers      = best_cfg["num_layers"],
                    dropout_p       = best_cfg["dropout_p"],
                )
            else:
                model = EnhancedTCNGNN(
                    input_size      = dm.train_dataset.features,
                    hidden_channels = best_cfg["hidden_size"],
                    output_dist     = "LogNormal",
                    n_stations      = dm.train_dataset.stations,
                    num_layers      = best_cfg["num_layers"],
                    kernel_size     = best_cfg["kernel_size"],
                    dropout_p       = best_cfg["dropout_p"],
                )

            mean_crps_h, mean_mae_h = evaluate_checkpoint(model, dm, edge_index, ckpt, mode)

            for h in horizons:
                scores_crps[h].append(mean_crps_h[h])
                scores_mae[h].append(mean_mae_h[h])

          # Now compute mean ± std
        summary_crps = {}
        summary_mae  = {}
        for h in horizons:
            m_crps = statistics.mean(scores_crps[h])
            s_crps = statistics.pstdev(scores_crps[h])   # population std
            summary_crps[h] = (m_crps, s_crps)

            m_mae  = statistics.mean(scores_mae[h])
            s_mae  = statistics.pstdev(scores_mae[h])
            summary_mae[h]  = (m_mae,  s_mae)

        overall_crps_per_seed = []
        overall_mae_per_seed  = []
        for i in range(n_seeds):
            crps_vals = [ scores_crps[h][i] for h in horizons ]
            mae_vals  = [ scores_mae[h][i]  for h in horizons ]
            overall_crps_per_seed.append( sum(crps_vals) / len(crps_vals) )
            overall_mae_per_seed .append( sum(mae_vals)  / len(mae_vals) )

        m_overall_crps = statistics.mean(overall_crps_per_seed)
        s_overall_crps = statistics.pstdev(overall_crps_per_seed)
        m_overall_mae  = statistics.mean(overall_mae_per_seed)
        s_overall_mae  = statistics.pstdev(overall_mae_per_seed)

        print(f"\n→ Mode = {mode} (averaged over {n_seeds} seeds)")
        print(" Lead |   CRPS   |    MAE")
        print(" ---------------------------")
        for h in horizons:
            m_crps, s_crps = summary_crps[h]
            m_mae,  s_mae  = summary_mae[h]
            print(f" {h:4d}h | {m_crps:6.4f}±{s_crps:.4f} | {m_mae:6.4f}±{s_mae:.4f}")
        print(f"\n→ Overall: CRPS = {m_overall_crps:.4f}±{s_overall_crps:.4f}, "
      f"MAE = {m_overall_mae:.4f}±{s_overall_mae:.4f}")
    print("\n" + "="*50)

"""# Diebold Mariano Test - You can skip this and see the thing before runs or not. Skip everything below this"""

"""Diebold–Mariano utilities for neural-forecast ensembles with analytic LogNormal CRPS on mean/std parameters
======================================================================

This module allows statistical comparison of probabilistic forecasts characterized
by their LogNormal parameters (mean=mu, stddev=sigma).

Key functions:
- `crps_lognormal_np`       : CRPS computed via analytic formula using numpy.
- `crps_params_per_seed_np` : Apply CRPS over ensembles of parameter sets.
- `dm_test`                 : Core Diebold–Mariano test on score arrays.
- `dm_test_from_params`     : High-level wrapper comparing two models given their mu/sigma arrays.

Shapes:
- mu, sigma: (K, n, H, N) or (n, H, N)
- obs      : (n, H, N) or (n, H, N, 1)
- CRPS     : same shape as mu/sigma input, minus singleton dims.
"""
from typing import Tuple, Optional, Literal
import numpy as np
from scipy import stats

# ----------------------------------------------------------------------------
# Analytic CRPS for LogNormal using numpy
# ----------------------------------------------------------------------------

def crps_lognormal_np(
    mu: np.ndarray,
    sigma: np.ndarray,
    y: np.ndarray,
) -> np.ndarray:
    """
    Compute CRPS for Y ~ LogNormal(mu, sigma) against observations y.
    mu, sigma: arrays of shape (n, H, N)
    y:          array of shape (n, H, N) (must be > 0)
    Returns:    array of shape (n, H, N)
    Formula:    omega = (ln y - mu)/sigma
                ex_input = clip(mu + 0.5 * sigma^2, max=15)
                ex = 2 * exp(ex_input)
                crps = y*(2*Phi(omega)-1) - ex*(Phi(omega - sigma) + Phi(sigma/sqrt(2)) -1)
    """
    # ensure positive
    eps = 1e-5
    mask = np.isfinite(y)

    # Replace non-finite values to avoid issues in np.log etc.
    y = np.where(mask, np.clip(y, eps, None), 1.0)  # use dummy value (e.g. 1.0) for masked y

    # squeeze trailing singleton dim
    if y.ndim > 3 and y.shape[-1] == 1:
        y = np.squeeze(y, axis=-1)
    if mu.ndim > 3 and mu.shape[-1] == 1:
        mu = np.squeeze(mu, axis=-1)
        sigma = np.squeeze(sigma, axis=-1)

    omega = (np.log(y) - mu) / sigma
    ex_input = np.clip(mu + 0.5 * sigma**2, None, 15.0)
    ex = 2.0 * np.exp(ex_input)

    Phi = stats.norm.cdf
    Phi_omega = Phi(omega)
    Phi_omega_m = Phi(omega - sigma)
    Phi_s = Phi(sigma / np.sqrt(2.0))

    crps = y * (2.0 * Phi_omega - 1.0) - ex * (Phi_omega_m + Phi_s - 1.0)
    return crps

# ----------------------------------------------------------------------------
# Apply CRPS over parameter ensembles for each seed
# ----------------------------------------------------------------------------

def crps_params_per_seed_np(
    mus: np.ndarray,
    sigmas: np.ndarray,
    y: np.ndarray,
) -> np.ndarray:
    """
    Compute CRPS separately for each seed given arrays of parameters.
    mus, sigmas: arrays of shape (K, n, H, N)
    y:          array of shape (n, H, N)
    Returns:    crps array of shape (K, n, H, N)
    """
    # prepare obs shape
    if y.ndim > 3 and y.shape[-1] == 1:
        y = np.squeeze(y, axis=-1)
    K = mus.shape[0] if mus.ndim == 4 else 1
    # handle single-seed case
    if mus.ndim == 3:
        return crps_lognormal_np(mus, sigmas, y)[None,...]
    # multi-seed
    crps_scores = []
    for k in range(K):
        crps_k = crps_lognormal_np(mus[k], sigmas[k], y)
        crps_scores.append(crps_k)
    return np.stack(crps_scores, axis=0)

# ----------------------------------------------------------------------------
# Core Diebold–Mariano statistic on numpy score arrays
# ----------------------------------------------------------------------------

def dm_test(
    score_f: np.ndarray,
    score_g: np.ndarray,
    axis: int = 0,
    power: Optional[int] = None,
    alternative: Literal['two-sided','less','greater']='two-sided',
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Perform Diebold–Mariano test comparing score_f and score_g.
    score_f, score_g: arrays of shape (n, H, N) or (n, H, N, 1)
    Returns t_stat, p_val of shape (H, N).
    """
    # convert to float
    sf = np.asarray(score_f, dtype=float)
    sg = np.asarray(score_g, dtype=float)
    # squeeze trailing singleton
    if sf.ndim > 3 and sf.shape[-1] == 1:
        sf = np.squeeze(sf, axis=-1)
        sg = np.squeeze(sg, axis=-1)
    if sf.shape != sg.shape:
        raise ValueError('score_f and score_g must match')
    # bring sample axis first
    sf = np.moveaxis(sf, axis, 0)
    sg = np.moveaxis(sg, axis, 0)
    n = sf.shape[0]
    if n < 2:
        raise ValueError('Need at least two samples for DM test')
    # elementwise loss diff
    if power is None:
        d = sf - sg
    else:
        d = np.abs(sf)**power - np.abs(sg)**power
    mean_d = d.mean(axis=0)
    var_d = (d**2).mean(axis=0)
    with np.errstate(divide='ignore', invalid='ignore'):
        t_stat = np.sqrt(n)*mean_d/np.sqrt(var_d)
    df = n - 1
    if alternative=='two-sided':
        p_val = 2*stats.t.sf(np.abs(t_stat), df)
    elif alternative=='less':
        p_val = stats.t.cdf(t_stat, df)
    else:
        p_val = stats.t.sf(t_stat, df)
    return t_stat, p_val

# ----------------------------------------------------------------------------
# Benjamini-Hochberg FDR
# ----------------------------------------------------------------------------

def benjamini_hochberg_pstar(pvals: np.ndarray, alpha: float=0.05) -> float:
    p = pvals.ravel()
    m = p.size
    order = np.argsort(p)
    thresh = alpha * (np.arange(1, m+1))/m
    below = p[order] <= thresh
    if not np.any(below): return 0.0
    return float(p[order][np.max(np.where(below))])

# ----------------------------------------------------------------------------
# High-level DM test comparing two models via their mu/sigma params
# ----------------------------------------------------------------------------

def dm_test_from_params(
    mus_f: np.ndarray,
    sigmas_f: np.ndarray,
    mus_g: np.ndarray,
    sigmas_g: np.ndarray,
    obs_unmasked: np.ndarray,
    obs_masked: np.ndarray,
    *,
    alpha: float=0.05,
    alternative: Literal['two-sided','less','greater']='two-sided',
    bh: bool=True,
    mask = [], # if 0 then model f gets masked, if 1 then model g gets masked, or both
) -> Tuple[np.ndarray, np.ndarray, Optional[float]]:
    """
    Compare two models characterized by mu/sigma arrays.
    mus_f/g, sigmas_f/g: (K, n, H, N)
    obs:              (n, H, N) or (n, H, N, 1)
    Returns t_stat, p_val arrays (H, N) and p_star.
    """
    # compute CRPS per seed (K,n,H,N)
    if 0 in mask:
        crps_f = crps_params_per_seed_np(mus_f, sigmas_f, obs_masked)
    else:
        crps_f = crps_params_per_seed_np(mus_f, sigmas_f, obs_unmasked)

    if 1 in mask:
        crps_g = crps_params_per_seed_np(mus_g, sigmas_g, obs_masked)
    else:
        crps_g = crps_params_per_seed_np(mus_g, sigmas_g, obs_unmasked)

    # average across seeds -> (n,H,N)
    score_f = crps_f.mean(axis=0)
    score_g = crps_g.mean(axis=0)
    # DM test
    t_stat, p_val = dm_test(score_f, score_g, axis=0, alternative=alternative)
    p_star = benjamini_hochberg_pstar(p_val, alpha) if bh else None
    return t_stat, p_val, p_star

import glob
import numpy as np
import torch
from torch.utils.data import DataLoader

# seeds and horizons
seeds    = [0,1,2,3,4]
horizons = [1,24,48,96]

# 1) load mus/sigmas from NPZs
def load_params(model_prefix, bg=False, masked=False):
    mus, sigs = [], []
    for seed in seeds:
        if not bg:
            print("Loading non bg version")
            fn = f"/content/drive/MyDrive/GDL/models_dist/{model_prefix}_{'masked' if masked is True else 'unmasked'}_seed{seed}_test_outputs.npz"
        else:
            print("Loading bg version")
            fn = f"/content/drive/MyDrive/GDL/models_dist/{model_prefix}_seed{seed}_base_graph_test_outputs.npz"
        data = np.load(fn)
        m = np.squeeze(data['mean'], axis=-1)
        s = np.squeeze(data['std'],  axis=-1)
        mus.append(m)
        sigs.append(s)
    mus = np.stack(mus, axis=0)
    sigs = np.stack(sigs, axis=0)
    return mus, sigs

mus_masked,   sigs_masked   = load_params("enhanced_tcn_gnn", masked=True)
mus_unmasked, sigs_unmasked = load_params("enhanced_tcn_gnn")

# 2) load the raw test observations
dm, _ = load_data()  # baseline ignores graph args
test_loader = DataLoader(dm.test_dataset, batch_size=32, shuffle=False, num_workers=4)

obs_unmasked_list = []
for _, y in test_loader:
    # y: [B, H, N, 1]
    obs_unmasked_list.append(y.cpu().numpy().squeeze(-1))  # -> (B, H, N)
obs_unmasked = np.concatenate(obs_unmasked_list, axis=0)  # (n, H, N)

# 3) build masked obs the same way you did in training
obs_masked = obs_unmasked.copy()

# Use compute_station_thresholds(dm) to mask the bad values per farm
mins, maxs = compute_station_thresholds(dm)
obs_masked = mask_anomalous_per_station(torch.tensor(obs_masked), mins, maxs).numpy()

# 4) run Diebold–Mariano: compare f=masked vs g=unmasked
#    we set mask=[0] so that f uses obs_masked, g uses obs_unmasked
t_stat, p_val, p_star = dm_test_from_params(
    mus_masked,   sigs_masked,
    mus_unmasked, sigs_unmasked,
    obs_unmasked, obs_masked,
    alternative='less',   # test if f has **lower** (better) CRPS than g
    bh=True               # apply Benjamini–Hochberg
)

# 5) report
print(f"\nOverall BH FDR threshold p* = {p_star:.3e}")
print(f"p-values   min = {p_val.min():.3e},   max = {p_val.max():.3e}")

# per-horizon station‐wise significance
print("\nLead | % stations with CRPS(unmasked) graph best hyperparam <CRPS(unmasked) base graph param at α=0.05")
print("----------------------------------------------------------")
for i, h in enumerate(horizons):
    # p_val has shape (H, N)
    p_h   = p_val[i]               # shape (N,)
    # compute BH threshold again for this horizon alone:
    _, p_star_h, rej_mask = benjamini_hochberg_pstar(p_h, alpha=0.05), None, None
    # but easier: reject where p_h <= global p_star
    rej_mask = (p_h <= p_star)
    frac = rej_mask.mean() * 100
    print(f" {h:3d}h |    {frac:5.1f}%")